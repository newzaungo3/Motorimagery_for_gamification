{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.datasets import eegbci\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf,read_raw_edf,read_raw_gdf,read_raw_fif\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.filter import construct_iir_filter,create_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from scipy.signal import iirfilter, sosfiltfilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\dataset\\\\physio\\\\MNE-eegbci-data\\\\files\\\\eegmmidb\\\\1.0.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/EEG_Model/physionet.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=82'>83</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_epoch,train_labels,valid_epoch,valid_labels,raw_epoch,raw\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=84'>85</a>\u001b[0m \u001b[39m#train_epoch,valid_epoch,train_labels,valid_labels,raw_epoch,raw= get_data()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=85'>86</a>\u001b[0m train_epoch,train_labels,valid_epoch,valid_labels,raw_epoch,raw\u001b[39m=\u001b[39m get_data()\n",
      "\u001b[1;32m/root/EEG_Model/physionet.ipynb Cell 2'\u001b[0m in \u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=73'>74</a>\u001b[0m     valid_path, train_path \u001b[39m=\u001b[39m get_physio()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=75'>76</a>\u001b[0m     \u001b[39m#tmin, tmax = -0.2, 0.4\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=76'>77</a>\u001b[0m     tmin, tmax \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m\n",
      "\u001b[1;32m/root/EEG_Model/physionet.ipynb Cell 2'\u001b[0m in \u001b[0;36mget_physio\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m subject \u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m01\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m02\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m03\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m04\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m05\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m06\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m07\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m08\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m09\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m10\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m11\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m12\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m13\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m14\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=2'>3</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdataset\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mphysio\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mMNE-eegbci-data\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfiles\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39meegmmidb\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1.0.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=3'>4</a>\u001b[0m folders \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(path)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=4'>5</a>\u001b[0m subject_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d6f746f72696d61676572795f666f725f67616d696669636174696f6e2d637564612d707974686f6e2d31227d/root/EEG_Model/physionet.ipynb#ch0000001vscode-remote?line=5'>6</a>\u001b[0m train_path \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\dataset\\\\physio\\\\MNE-eegbci-data\\\\files\\\\eegmmidb\\\\1.0.0'"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_physio():\n",
    "    subject =['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "    path = \"EEG_Model\\\\dataset\\\\physio\\\\MNE-eegbci-data\\\\files\\\\eegmmidb\\\\1.0.0\"\n",
    "    folders = os.listdir(path)\n",
    "    subject_count = 0\n",
    "    train_path = []\n",
    "    valid_path = []\n",
    "    count = 0\n",
    "    for fol in folders:\n",
    "        if count == 10:\n",
    "            break\n",
    "        else:\n",
    "            count +=1\n",
    "        for i in range (len (subject)):\n",
    "            if  subject[i] in ['01','02','05','06','09','10','13','14']:\n",
    "                pass\n",
    "            elif subject[i] in ['03','04','07','08']:\n",
    "                file = \"EEG_Model\\\\dataset\\\\physio\\\\MNE-eegbci-data\\\\files\\\\eegmmidb\\\\1.0.0\\\\{}\\\\{}R{}.edf\".format(fol,fol, subject[i])\n",
    "                train_path.append(file)\n",
    "            elif subject[i] in ['11','12']:\n",
    "                file = \"EEG_Model\\\\dataset\\\\physio\\\\MNE-eegbci-data\\\\files\\\\eegmmidb\\\\1.0.0\\\\{}\\\\{}R{}.edf\".format(fol,fol, subject[i])\n",
    "                valid_path.append(file)\n",
    "            \n",
    "  \n",
    "    return valid_path,train_path\n",
    "\n",
    "def get_epoch(data_path,tmin,tmax,event_id,preprocess=False,ica=False):\n",
    "    \n",
    "    raw = concatenate_raws([read_raw_edf(f, preload=True,verbose='WARNING') for f in data_path])\n",
    "    raw_data = raw.copy()\n",
    "    eegbci.standardize(raw_data)\n",
    "    montage = mne.channels.make_standard_montage('standard_1005')\n",
    "    raw_data.set_montage(montage)\n",
    "    print(raw_data.info['ch_names'])\n",
    "    raw_data.rename_channels(lambda x: x.strip('.'))\n",
    "    print(raw_data.info['ch_names'])\n",
    "    print(raw_data.info['sfreq'])\n",
    "    \n",
    "    sfreq = 641\n",
    "    nyq = sfreq / 2 \n",
    "    f_p = 40.\n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    if preprocess == True:\n",
    "        iir_param = dict(order=6, ftype='butter', output='sos')\n",
    "        #iir_param = construct_iir_filter(iir_param, 40, None, 1000, 'low', return_copy=False) \n",
    "         \n",
    "        #raw_data.filter(l_freq=0.05, h_freq=40.,fir_design='firwin', verbose=20)\n",
    "        raw_data.filter(l_freq=0.05, h_freq=75.,method = 'iir',iir_params=iir_param,phase='zero')\n",
    "        #raw_data.notch_filter(60,filter_length='auto', phase='zero')\n",
    "        raw_data.notch_filter(50,filter_length='auto', phase='zero')\n",
    "        \n",
    "    if ica == True:\n",
    "        ica = mne.preprocessing.ICA(n_components=64, max_iter=100)\n",
    "        ica.fit(raw_data)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw_data, picks=ica.exclude)\n",
    "        ica.apply(raw_data)\n",
    "    #2 electrode        \n",
    "    #raw_data.pick_channels(['C3','C4'])\n",
    "    #16 electrode\n",
    "    #raw_data.pick_channels(['FC3','FCz','FC5','C1','C2','C3','C4','C5','C6','Cz','CP3','CPz','CP4','P3','Pz','P4'])\n",
    "    events, event_id = events_from_annotations(raw_data,event_id=event_id)\n",
    "    picks = pick_types(raw_data.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                       exclude='bads')\n",
    "    reject_criteria = dict(eeg=100e-6)  #most frequency in this range is not brain components\n",
    "    \n",
    "    epochs = Epochs(raw_data, events, event_id, tmin, tmax, proj=True, picks=picks,\n",
    "                    baseline=None,preload=True)\n",
    "    labels = epochs.events[:, -1]\n",
    "    return epochs.get_data(),labels,epochs,raw_data\n",
    "    \n",
    "def get_data():\n",
    "    valid_path, train_path = get_physio()\n",
    "        \n",
    "    #tmin, tmax = -0.2, 0.4\n",
    "    tmin, tmax = 0, 4\n",
    "    event_id = dict(T1=0, T2=1)\n",
    "\n",
    "    train_epoch,train_labels,raw_epoch,raw = get_epoch(train_path,tmin,tmax,event_id,False)\n",
    "    valid_epoch,valid_labels,_,_ = get_epoch(valid_path,tmin,tmax,event_id,False)\n",
    "    \n",
    "    return train_epoch,train_labels,valid_epoch,valid_labels,raw_epoch,raw\n",
    "\n",
    "#train_epoch,valid_epoch,train_labels,valid_labels,raw_epoch,raw= get_data()\n",
    "train_epoch,train_labels,valid_epoch,valid_labels,raw_epoch,raw= get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 64, 641)\n",
      "(600,)\n",
      "---------------\n",
      "(600, 1, 64, 641)\n",
      "64\n",
      "[1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1\n",
      " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
      " 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
      " 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
      " 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1\n",
      " 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
      " 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
      " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1\n",
      " 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0\n",
      " 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1\n",
      " 0 1 0 1 0 0 1 0]\n",
      "['FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'Fp1', 'Fpz', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FT8', 'T7', 'T8', 'T9', 'T10', 'TP7', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'O1', 'Oz', 'O2', 'Iz']\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# 100068 events(epoch)\n",
    "# 64 channel\n",
    "# 961 Time(samples)\n",
    "#(event,channel,time)\n",
    "print(train_epoch.shape)\n",
    "print(train_labels.shape)\n",
    "print('---------------')\n",
    "#print(valid_epoch.shape)\n",
    "#print(valid_labels.shape)\n",
    "X = train_epoch[:, np.newaxis,:,:]\n",
    "y = train_labels\n",
    "print(X.shape)\n",
    "print(train_epoch.shape[1])\n",
    "print(train_labels)\n",
    "print(raw.info['ch_names'])\n",
    "print(raw.info['nchan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\nvalid , train = get_physio()\\nfilename = 'validpath'\\noutfile = open(filename,'wb')\\npickle.dump(valid,outfile)\\noutfile.close()\\ninfile = open(filename,'rb')\\nnew = pickle.load(infile)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "valid , train = get_physio()\n",
    "filename = 'validpath'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(valid,outfile)\n",
    "outfile.close()\n",
    "infile = open(filename,'rb')\n",
    "new = pickle.load(infile)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1, 64, 641)\n",
      "(600,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#10614 data,64 channel,161 timesamples\n",
    "\n",
    "X_t,y_train= train_epoch.copy(),train_labels\n",
    "\n",
    "X_val,y_valid = valid_epoch.copy(),valid_labels\n",
    "\n",
    "'''X = X[:, np.newaxis,:,:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)'''\n",
    "\n",
    "#add new dimension\n",
    "X_train = X_t[:, np.newaxis,:,:]\n",
    "X_valid = X_val[:,np.newaxis,:,:]\n",
    "\n",
    "'''\n",
    "X_train = X_t[:,:,:]\n",
    "X_valid = X_val[:,:,:]\n",
    "'''\n",
    "#X_valid = X_valid[:,np.newaxis,:,:]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "from torch.utils.data import Subset  \n",
    "from torch import nn  \n",
    "import torch.nn.functional as F  \n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.temporal= nn.Conv1d(in_channels=2,out_channels=25,kernel_size=1,stride=1,padding=1)\n",
    "        \n",
    "        self.spatial = nn.Conv1d(in_channels=25,out_channels=25,kernel_size=25,stride=1,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(25,False)\n",
    "        self.maxPooling = nn.MaxPool1d(3)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=25,out_channels=50,kernel_size=11,stride=1,padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=50,out_channels=100,kernel_size=11,stride=1,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(100,False)\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(in_channels=100,out_channels=200,kernel_size=11,stride=1,padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200,False)\n",
    "        self.maxPooling2 = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fully = nn.Linear(1000,2) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x = self.temporal(x)\n",
    "        #print('Temporal: '+str(x.shape))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x,0.5)\n",
    "        #print('Dropout: '+str(x.shape))\n",
    "        \n",
    "        x = self.spatial(x)\n",
    "        #print('Spatial: '+str(x.shape))\n",
    "        x = self.batchnorm1(x)\n",
    "        #print('Batchnorm: '+str(x.shape))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.maxPooling(x)\n",
    "        #print('Max pooling: '+str(x.shape))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        #print('Convo3 : '+str(x.shape))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x,0.5)\n",
    "        x = self.maxPooling(x)\n",
    "        #rint('Max pooling : '+str(x.shape))\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        #print('Convo4 : '+str(x.shape))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x,0.5)\n",
    "        #print('Dropout: '+str(x.shape))\n",
    "        x = self.maxPooling(x)\n",
    "        #print('Max pooling : '+str(x.shape))\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        #print('Convo5 : '+str(x.shape))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.maxPooling2(x)\n",
    "        #print('Max pooling 2 : '+str(x.shape))\n",
    "        x = self.flatten(x)\n",
    "        #print('Flatten : '+str(x.shape))\n",
    "        out = self.fully(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #in_channel = 1\n",
    "            #out_channel = 16\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #after layer 1 will be of shape [32, 16, 32, 80]\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        #add\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        #nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        #after layer 2 will be of shape [100, 32, 16, 40]\n",
    "        #20480   40960\n",
    "        self.fc = nn.Linear(327680, 2)\n",
    "        self.drop_out = nn.Dropout(0.5)  #zeroed 0.2% data\n",
    "        #after fc will be of shape [100, 10]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shape: [batch, in_channel, img_width, img_height]\n",
    "        #[32, 1, 64, 161]\n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape)\n",
    "        out = self.drop_out(out)\n",
    "        #after layer 1: shape: [32, 16, 32, 80]\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.drop_out(out)\n",
    "        #after layer 2: shape: [100, 32, 16, 40]\n",
    "        out = out.reshape(out.size(0), -1)   #can also use .view()\n",
    "        #after squeezing: shape: [32, 20480]\n",
    "        #print(out.shape)\n",
    "        #we squeeze so that it can be inputted into the fc layer\n",
    "        out = self.fc(out)\n",
    "        #after fc layer: shape: [32, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #in_channel = 1\n",
    "            #out_channel = 25\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv2d(1,25,kernel_size = 3,stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(0.5)  \n",
    "            )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            #in_channel = 1\n",
    "            #out_channel = 16\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv2d(25,25,kernel_size = 4, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(25,False),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(25,50,kernel_size=3,stride=1,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(0.5)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(50,100,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(100,False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(0.5)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(100,200,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(200,False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(0.5)\n",
    "        )\n",
    "        \n",
    "        self.fully = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2400,2)\n",
    "        )\n",
    "        \n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=1,stride = 3)\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=1,stride = 2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        #print(\"layer1: \" + str(out.shape))\n",
    "        out = self.layer2(out)\n",
    "        #print(\"layer2: \" + str(out.shape))\n",
    "        out = self.maxpooling1(out)\n",
    "        #print(\"Maxpooling: \" + str(out.shape))\n",
    "        out = self.layer3(out)\n",
    "        #print(\"layer3: \" + str(out.shape))\n",
    "        out = self.maxpooling1(out)\n",
    "        #print(\"Maxpooling: \" + str(out.shape))\n",
    "        out = self.layer4(out)\n",
    "        #print(\"layer4: \" + str(out.shape))\n",
    "        out = self.maxpooling1(out)\n",
    "        #print(\"Maxpooling: \" + str(out.shape))\n",
    "        out = self.layer5(out)\n",
    "        #print(\"layer5: \" + str(out.shape))\n",
    "        out = self.maxpooling2(out)\n",
    "        #print(\"Maxpooling: \" + str(out.shape))\n",
    "        out = self.fully(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 1d convolution and 2d input (channel,timewindow)\n",
    "class hopefullnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(hopefullnet,self).__init__()\n",
    "        \n",
    "        self.L1 = nn.Sequential(\n",
    "            #in_channel = 2\n",
    "            #out_channel or Filter size = 32\n",
    "            #kernel size = 20\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv1d(2,32, kernel_size = 20, stride = 1 , padding = 'same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "        self.L2 = nn.Sequential(\n",
    "            #in_channel = 32\n",
    "            #out_channel or Filter size = 32\n",
    "            #kernel_size =20\n",
    "            #stride = 1\n",
    "            #padding = valid\n",
    "            #Relu\n",
    "            nn.Conv1d(32,32,kernel_size=20,stride = 1, padding = 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "        self.L3 = nn.Conv1d(32,32,kernel_size=6,stride = 1, padding='valid')\n",
    "        self.pooling = nn.AvgPool1d(kernel_size=2,stride = 2)\n",
    "        self.L4 = nn.Sequential(\n",
    "            #in_channel = 32\n",
    "            #out_channel = 32\n",
    "            #kernel_size = 6\n",
    "            #stride = 1\n",
    "            #padding = 'valid'\n",
    "            #relu\n",
    "            nn.Conv1d(32,32,kernel_size = 6, stride = 1 , padding = 'valid'),\n",
    "        )\n",
    "        \n",
    "        self.fully = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(nn.Linear(9696,2)),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(nn.Linear(296,148)),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(nn.Linear(148,74)),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(74,2)\n",
    "        )\n",
    "        self.fully1 = nn.Linear(9696,296)\n",
    "        self.fully2 = nn.Linear(296,148)\n",
    "        self.fully3 = nn.Linear(148,74)\n",
    "        self.fully4 = nn.Linear(74,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.L1(x)\n",
    "        out = self.L2(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.L3(out)\n",
    "        out = self.pooling(out)\n",
    "        out = self.L4(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = F.relu(self.fully1(out))\n",
    "        out = self.drop_out(out)\n",
    "        out = F.relu(self.fully2(out))\n",
    "        out = self.drop_out(out)\n",
    "        out = F.relu(self.fully3(out))\n",
    "        out = self.drop_out(out)\n",
    "        out = F.softmax(self.fully4(out))\n",
    "        #out = self.fully(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 2d convolution and 3d input (1,channel,timewindow)\n",
    "class gamenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gamenet,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            #in_channel = 16\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (1,25)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(1,100,kernel_size=(1,25),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (16,1)\n",
    "            #stride = 1\n",
    "            #padding = Valid\n",
    "            #Relu\n",
    "            #nn.Conv2d(100,100,kernel_size=(16,1),stride=1,padding='valid')\n",
    "            nn.Conv2d(100,100,kernel_size=(64,1),stride=1,padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(100,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=(1,7),stride=5)\n",
    "        self.l4 = nn.Sequential(\n",
    "            #in_channel = 50\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(50,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=(1,3),stride=2)\n",
    "        \n",
    "\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(3150),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(3150,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc5 = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.softmax = nn.Sequential(\n",
    "            nn.Linear(32,2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.maxpooling1(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.maxpooling2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "batch_size = 32\n",
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "train_iterator = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "valid_iterator = create_dataloader(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_iterator.dataset) / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py:439: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ..\\aten\\src\\ATen\\native\\Convolution.cpp:660.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "C:\\Users\\asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 100, 64, 641]           2,600\n",
      "              ReLU-2         [32, 100, 64, 641]               0\n",
      "       BatchNorm2d-3         [32, 100, 64, 641]             200\n",
      "            Conv2d-4          [32, 100, 1, 641]         640,100\n",
      "              ReLU-5          [32, 100, 1, 641]               0\n",
      "       BatchNorm2d-6          [32, 100, 1, 641]             200\n",
      "            Conv2d-7           [32, 50, 1, 641]         150,050\n",
      "              ReLU-8           [32, 50, 1, 641]               0\n",
      "       BatchNorm2d-9           [32, 50, 1, 641]             100\n",
      "        MaxPool2d-10           [32, 50, 1, 127]               0\n",
      "           Conv2d-11           [32, 50, 1, 127]          75,050\n",
      "             ReLU-12           [32, 50, 1, 127]               0\n",
      "      BatchNorm2d-13           [32, 50, 1, 127]             100\n",
      "        MaxPool2d-14            [32, 50, 1, 63]               0\n",
      "          Flatten-15                 [32, 3150]               0\n",
      "      BatchNorm1d-16                 [32, 3150]           6,300\n",
      "          Dropout-17                 [32, 3150]               0\n",
      "           Linear-18                 [32, 1024]       3,226,624\n",
      "             ReLU-19                 [32, 1024]               0\n",
      "      BatchNorm1d-20                 [32, 1024]           2,048\n",
      "          Dropout-21                 [32, 1024]               0\n",
      "           Linear-22                  [32, 512]         524,800\n",
      "             ReLU-23                  [32, 512]               0\n",
      "      BatchNorm1d-24                  [32, 512]           1,024\n",
      "          Dropout-25                  [32, 512]               0\n",
      "           Linear-26                  [32, 256]         131,328\n",
      "             ReLU-27                  [32, 256]               0\n",
      "      BatchNorm1d-28                  [32, 256]             512\n",
      "          Dropout-29                  [32, 256]               0\n",
      "           Linear-30                  [32, 128]          32,896\n",
      "             ReLU-31                  [32, 128]               0\n",
      "      BatchNorm1d-32                  [32, 128]             256\n",
      "          Dropout-33                  [32, 128]               0\n",
      "           Linear-34                   [32, 64]           8,256\n",
      "             ReLU-35                   [32, 64]               0\n",
      "      BatchNorm1d-36                   [32, 64]             128\n",
      "          Dropout-37                   [32, 64]               0\n",
      "           Linear-38                   [32, 32]           2,080\n",
      "             ReLU-39                   [32, 32]               0\n",
      "      BatchNorm1d-40                   [32, 32]              64\n",
      "          Dropout-41                   [32, 32]               0\n",
      "           Linear-42                    [32, 2]              66\n",
      "          Softmax-43                    [32, 2]               0\n",
      "================================================================\n",
      "Total params: 4,804,782\n",
      "Trainable params: 4,804,782\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5.01\n",
      "Forward/backward pass size (MB): 3086.36\n",
      "Params size (MB): 18.33\n",
      "Estimated Total Size (MB): 3109.69\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = 'cuda'\n",
    "#model = CNN()\n",
    "#model = CNN2D()\n",
    "#model = ConvNet()\n",
    "#model = hopefullnet()\n",
    "model = gamenet()\n",
    "#print(model)\n",
    "\n",
    "#summary(model.cuda(), ( 2 , 641),32)\n",
    "summary(model.cuda(), ( 1,64, 641),32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i, (x,y) in enumerate(train_iterator):\\n    x = x.to(device=device,dtype=torch.float32)\\n    model.to(device=device) \\n    print(x.shape)\\n    model(x)\\n    \\n    break'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i, (x,y) in enumerate(train_iterator):\n",
    "    x = x.to(device=device,dtype=torch.float32)\n",
    "    model.to(device=device) \n",
    "    print(x.shape)\n",
    "    model(x)\n",
    "    \n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_torch = torch.from_numpy(X_train).double()\\ny_torch = torch.from_numpy(y_train).long()\\ntrain_set = TensorDataset(X_torch, y_torch)\\n#valid_set = TensorDataset(torch.from_numpy(X_valid).double(),torch.from_numpy(y_valid).long())\\n\\nBATCH_SIZE = 32\\ntrain_iterator = torch.utils.data.DataLoader(dataset=train_set, \\n                                           batch_size=BATCH_SIZE, \\n                                           shuffle=True)\\n\\nvalid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \\n                                           batch_size=BATCH_SIZE, \\n                                           shuffle=True)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''X_torch = torch.from_numpy(X_train).double()\n",
    "y_torch = torch.from_numpy(y_train).long()\n",
    "train_set = TensorDataset(X_torch, y_torch)\n",
    "#valid_set = TensorDataset(torch.from_numpy(X_valid).double(),torch.from_numpy(y_valid).long())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_iterator = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "\n",
    "valid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnewturno\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "C:\\Users\\asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\asus\\Desktop\\Motorimagery_for_gamification\\wandb\\run-20220315_110013-k2zsiare</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/newturno/Motor-Imagery/runs/k2zsiare\" target=\"_blank\">Gamenet_64Elec_executedImagine_Wandb</a></strong> to <a href=\"https://wandb.ai/newturno/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"Gamenet_64Elec_executedImagine_Wandb\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.00001,\n",
    "      \"architecture\": \"GameNet\",\n",
    "      \"dataset\": \"Physionet\",\n",
    "      \"epochs\": 500,\n",
    "      \"weightname\":\"Physionet_Gamenet_64elec_executedImagine_Wandb\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "config = wand.config\n",
    "print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Tr Loss: 0.7017, Tr Acc: 50.6667, Val Loss: 0.6979, Val Acc: 51.0000\n",
      "\n",
      "Starting epoch 2 / 500\n",
      "Epoch 2/500, Tr Loss: 0.7013, Tr Acc: 51.1667, Val Loss: 0.7115, Val Acc: 51.0000\n",
      "\n",
      "Starting epoch 3 / 500\n",
      "Epoch 3/500, Tr Loss: 0.7036, Tr Acc: 51.5000, Val Loss: 0.7141, Val Acc: 51.0000\n",
      "\n",
      "Starting epoch 4 / 500\n",
      "Epoch 4/500, Tr Loss: 0.7035, Tr Acc: 50.8333, Val Loss: 0.7153, Val Acc: 51.3333\n",
      "\n",
      "Starting epoch 5 / 500\n",
      "Epoch 5/500, Tr Loss: 0.7023, Tr Acc: 52.8333, Val Loss: 0.7032, Val Acc: 51.0000\n",
      "\n",
      "Starting epoch 6 / 500\n",
      "Epoch 6/500, Tr Loss: 0.6903, Tr Acc: 54.3333, Val Loss: 0.6974, Val Acc: 52.0000\n",
      "\n",
      "Starting epoch 7 / 500\n",
      "Epoch 7/500, Tr Loss: 0.6827, Tr Acc: 55.5000, Val Loss: 0.6911, Val Acc: 49.6667\n",
      "\n",
      "Starting epoch 8 / 500\n",
      "Epoch 8/500, Tr Loss: 0.6818, Tr Acc: 56.0000, Val Loss: 0.6872, Val Acc: 54.3333\n",
      "\n",
      "Starting epoch 9 / 500\n",
      "Epoch 9/500, Tr Loss: 0.6766, Tr Acc: 58.1667, Val Loss: 0.6855, Val Acc: 55.0000\n",
      "\n",
      "Starting epoch 10 / 500\n",
      "Epoch 10/500, Tr Loss: 0.6684, Tr Acc: 59.8333, Val Loss: 0.6920, Val Acc: 53.3333\n",
      "\n",
      "Starting epoch 11 / 500\n",
      "Epoch 11/500, Tr Loss: 0.6747, Tr Acc: 58.6667, Val Loss: 0.6792, Val Acc: 53.0000\n",
      "\n",
      "Starting epoch 12 / 500\n",
      "Epoch 12/500, Tr Loss: 0.6717, Tr Acc: 60.0000, Val Loss: 0.6870, Val Acc: 52.3333\n",
      "\n",
      "Starting epoch 13 / 500\n",
      "Epoch 13/500, Tr Loss: 0.6717, Tr Acc: 58.0000, Val Loss: 0.6765, Val Acc: 57.3333\n",
      "\n",
      "Starting epoch 14 / 500\n",
      "Epoch 14/500, Tr Loss: 0.6480, Tr Acc: 64.8333, Val Loss: 0.6652, Val Acc: 61.6667\n",
      "\n",
      "Starting epoch 15 / 500\n",
      "Epoch 15/500, Tr Loss: 0.6468, Tr Acc: 63.6667, Val Loss: 0.6655, Val Acc: 67.6667\n",
      "\n",
      "Starting epoch 16 / 500\n",
      "Epoch 16/500, Tr Loss: 0.6434, Tr Acc: 64.1667, Val Loss: 0.6617, Val Acc: 64.3333\n",
      "\n",
      "Starting epoch 17 / 500\n",
      "Epoch 17/500, Tr Loss: 0.6478, Tr Acc: 61.5000, Val Loss: 0.6648, Val Acc: 62.6667\n",
      "\n",
      "Starting epoch 18 / 500\n",
      "Epoch 18/500, Tr Loss: 0.6346, Tr Acc: 67.1667, Val Loss: 0.6564, Val Acc: 61.3333\n",
      "\n",
      "Starting epoch 19 / 500\n",
      "Epoch 19/500, Tr Loss: 0.6296, Tr Acc: 67.1667, Val Loss: 0.6584, Val Acc: 60.6667\n",
      "\n",
      "Starting epoch 20 / 500\n",
      "Epoch 20/500, Tr Loss: 0.6228, Tr Acc: 68.5000, Val Loss: 0.6449, Val Acc: 68.6667\n",
      "\n",
      "Starting epoch 21 / 500\n",
      "Epoch 21/500, Tr Loss: 0.6177, Tr Acc: 70.3333, Val Loss: 0.6218, Val Acc: 71.0000\n",
      "\n",
      "Starting epoch 22 / 500\n",
      "Epoch 22/500, Tr Loss: 0.6046, Tr Acc: 74.0000, Val Loss: 0.6609, Val Acc: 60.0000\n",
      "\n",
      "Starting epoch 23 / 500\n",
      "Epoch 23/500, Tr Loss: 0.6023, Tr Acc: 73.0000, Val Loss: 0.6324, Val Acc: 66.6667\n",
      "\n",
      "Starting epoch 24 / 500\n",
      "Epoch 24/500, Tr Loss: 0.6039, Tr Acc: 70.8333, Val Loss: 0.6373, Val Acc: 66.6667\n",
      "\n",
      "Starting epoch 25 / 500\n",
      "Epoch 25/500, Tr Loss: 0.5909, Tr Acc: 74.5000, Val Loss: 0.6461, Val Acc: 66.6667\n",
      "\n",
      "Starting epoch 26 / 500\n",
      "Epoch 26/500, Tr Loss: 0.5940, Tr Acc: 72.8333, Val Loss: 0.6509, Val Acc: 63.0000\n",
      "\n",
      "Starting epoch 27 / 500\n",
      "Epoch 27/500, Tr Loss: 0.5854, Tr Acc: 75.6667, Val Loss: 0.6319, Val Acc: 66.6667\n",
      "\n",
      "Starting epoch 28 / 500\n",
      "Epoch 28/500, Tr Loss: 0.5735, Tr Acc: 76.8333, Val Loss: 0.6410, Val Acc: 64.0000\n",
      "\n",
      "Starting epoch 29 / 500\n",
      "Epoch 29/500, Tr Loss: 0.5725, Tr Acc: 79.3333, Val Loss: 0.6383, Val Acc: 66.3333\n",
      "\n",
      "Starting epoch 30 / 500\n",
      "Epoch 30/500, Tr Loss: 0.5760, Tr Acc: 77.6667, Val Loss: 0.6167, Val Acc: 69.3333\n",
      "\n",
      "Starting epoch 31 / 500\n",
      "Epoch 31/500, Tr Loss: 0.5624, Tr Acc: 79.8333, Val Loss: 0.5997, Val Acc: 72.6667\n",
      "\n",
      "Starting epoch 32 / 500\n",
      "Epoch 32/500, Tr Loss: 0.5538, Tr Acc: 82.0000, Val Loss: 0.6134, Val Acc: 72.3333\n",
      "\n",
      "Starting epoch 33 / 500\n",
      "Epoch 33/500, Tr Loss: 0.5604, Tr Acc: 80.5000, Val Loss: 0.5834, Val Acc: 77.3333\n",
      "\n",
      "Starting epoch 34 / 500\n",
      "Epoch 34/500, Tr Loss: 0.5619, Tr Acc: 79.0000, Val Loss: 0.6150, Val Acc: 70.3333\n",
      "\n",
      "Starting epoch 35 / 500\n",
      "Epoch 35/500, Tr Loss: 0.5487, Tr Acc: 80.6667, Val Loss: 0.6062, Val Acc: 71.0000\n",
      "\n",
      "Starting epoch 36 / 500\n",
      "Epoch 36/500, Tr Loss: 0.5408, Tr Acc: 82.8333, Val Loss: 0.6043, Val Acc: 71.3333\n",
      "\n",
      "Starting epoch 37 / 500\n",
      "Epoch 37/500, Tr Loss: 0.5478, Tr Acc: 81.6667, Val Loss: 0.5886, Val Acc: 75.6667\n",
      "\n",
      "Starting epoch 38 / 500\n",
      "Epoch 38/500, Tr Loss: 0.5490, Tr Acc: 81.5000, Val Loss: 0.5803, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 39 / 500\n",
      "Epoch 39/500, Tr Loss: 0.5355, Tr Acc: 85.1667, Val Loss: 0.5695, Val Acc: 76.6667\n",
      "\n",
      "Starting epoch 40 / 500\n",
      "Epoch 40/500, Tr Loss: 0.5346, Tr Acc: 83.3333, Val Loss: 0.5621, Val Acc: 77.6667\n",
      "\n",
      "Starting epoch 41 / 500\n",
      "Epoch 41/500, Tr Loss: 0.5231, Tr Acc: 87.5000, Val Loss: 0.5756, Val Acc: 78.0000\n",
      "\n",
      "Starting epoch 42 / 500\n",
      "Epoch 42/500, Tr Loss: 0.5205, Tr Acc: 85.0000, Val Loss: 0.5814, Val Acc: 77.3333\n",
      "\n",
      "Starting epoch 43 / 500\n",
      "Epoch 43/500, Tr Loss: 0.5246, Tr Acc: 85.0000, Val Loss: 0.5651, Val Acc: 78.6667\n",
      "\n",
      "Starting epoch 44 / 500\n",
      "Epoch 44/500, Tr Loss: 0.5202, Tr Acc: 85.1667, Val Loss: 0.5566, Val Acc: 78.6667\n",
      "\n",
      "Starting epoch 45 / 500\n",
      "Epoch 45/500, Tr Loss: 0.5225, Tr Acc: 84.5000, Val Loss: 0.5676, Val Acc: 76.6667\n",
      "\n",
      "Starting epoch 46 / 500\n",
      "Epoch 46/500, Tr Loss: 0.5204, Tr Acc: 84.3333, Val Loss: 0.5718, Val Acc: 74.0000\n",
      "\n",
      "Starting epoch 47 / 500\n",
      "Epoch 47/500, Tr Loss: 0.5163, Tr Acc: 85.8333, Val Loss: 0.5798, Val Acc: 74.3333\n",
      "\n",
      "Starting epoch 48 / 500\n",
      "Epoch 48/500, Tr Loss: 0.5071, Tr Acc: 88.0000, Val Loss: 0.5916, Val Acc: 71.6667\n",
      "\n",
      "Starting epoch 49 / 500\n",
      "Epoch 49/500, Tr Loss: 0.5090, Tr Acc: 85.1667, Val Loss: 0.5710, Val Acc: 75.0000\n",
      "\n",
      "Starting epoch 50 / 500\n",
      "Epoch 50/500, Tr Loss: 0.4884, Tr Acc: 89.0000, Val Loss: 0.5608, Val Acc: 77.6667\n",
      "\n",
      "Starting epoch 51 / 500\n",
      "Epoch 51/500, Tr Loss: 0.5048, Tr Acc: 87.0000, Val Loss: 0.5632, Val Acc: 73.3333\n",
      "\n",
      "Starting epoch 52 / 500\n",
      "Epoch 52/500, Tr Loss: 0.4942, Tr Acc: 89.1667, Val Loss: 0.5738, Val Acc: 76.6667\n",
      "\n",
      "Starting epoch 53 / 500\n",
      "Epoch 53/500, Tr Loss: 0.4908, Tr Acc: 89.5000, Val Loss: 0.5792, Val Acc: 75.0000\n",
      "\n",
      "Starting epoch 54 / 500\n",
      "Epoch 54/500, Tr Loss: 0.4901, Tr Acc: 89.0000, Val Loss: 0.5673, Val Acc: 75.6667\n",
      "\n",
      "Starting epoch 55 / 500\n",
      "Epoch 55/500, Tr Loss: 0.4872, Tr Acc: 89.6667, Val Loss: 0.5708, Val Acc: 75.6667\n",
      "\n",
      "Starting epoch 56 / 500\n",
      "Epoch 56/500, Tr Loss: 0.4831, Tr Acc: 88.8333, Val Loss: 0.5598, Val Acc: 76.3333\n",
      "\n",
      "Starting epoch 57 / 500\n",
      "Epoch 57/500, Tr Loss: 0.4766, Tr Acc: 89.8333, Val Loss: 0.5724, Val Acc: 75.0000\n",
      "\n",
      "Starting epoch 58 / 500\n",
      "Epoch 58/500, Tr Loss: 0.4815, Tr Acc: 89.3333, Val Loss: 0.5635, Val Acc: 75.6667\n",
      "\n",
      "Starting epoch 59 / 500\n",
      "Epoch 59/500, Tr Loss: 0.4739, Tr Acc: 91.5000, Val Loss: 0.5715, Val Acc: 75.6667\n",
      "\n",
      "Starting epoch 60 / 500\n",
      "Epoch 60/500, Tr Loss: 0.4713, Tr Acc: 90.8333, Val Loss: 0.5704, Val Acc: 76.3333\n",
      "\n",
      "Starting epoch 61 / 500\n",
      "Epoch 61/500, Tr Loss: 0.4810, Tr Acc: 91.0000, Val Loss: 0.5631, Val Acc: 77.3333\n",
      "\n",
      "Starting epoch 62 / 500\n",
      "Epoch 62/500, Tr Loss: 0.4736, Tr Acc: 89.8333, Val Loss: 0.5560, Val Acc: 78.0000\n",
      "\n",
      "Starting epoch 63 / 500\n",
      "Epoch 63/500, Tr Loss: 0.4803, Tr Acc: 88.5000, Val Loss: 0.5397, Val Acc: 78.6667\n",
      "\n",
      "Starting epoch 64 / 500\n",
      "Epoch 64/500, Tr Loss: 0.4653, Tr Acc: 91.5000, Val Loss: 0.5359, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 65 / 500\n",
      "Epoch 65/500, Tr Loss: 0.4671, Tr Acc: 90.8333, Val Loss: 0.5616, Val Acc: 77.0000\n",
      "\n",
      "Starting epoch 66 / 500\n",
      "Epoch 66/500, Tr Loss: 0.4527, Tr Acc: 93.1667, Val Loss: 0.5673, Val Acc: 76.6667\n",
      "\n",
      "Starting epoch 67 / 500\n",
      "Epoch 67/500, Tr Loss: 0.4685, Tr Acc: 90.5000, Val Loss: 0.5581, Val Acc: 77.3333\n",
      "\n",
      "Starting epoch 68 / 500\n",
      "Epoch 68/500, Tr Loss: 0.4539, Tr Acc: 93.0000, Val Loss: 0.5769, Val Acc: 74.0000\n",
      "\n",
      "Starting epoch 69 / 500\n",
      "Epoch 69/500, Tr Loss: 0.4512, Tr Acc: 93.5000, Val Loss: 0.5406, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 70 / 500\n",
      "Epoch 70/500, Tr Loss: 0.4484, Tr Acc: 92.6667, Val Loss: 0.5343, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 71 / 500\n",
      "Epoch 71/500, Tr Loss: 0.4640, Tr Acc: 90.1667, Val Loss: 0.5368, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 72 / 500\n",
      "Epoch 72/500, Tr Loss: 0.4476, Tr Acc: 92.8333, Val Loss: 0.5403, Val Acc: 79.3333\n",
      "\n",
      "Starting epoch 73 / 500\n",
      "Epoch 73/500, Tr Loss: 0.4447, Tr Acc: 94.1667, Val Loss: 0.5398, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 74 / 500\n",
      "Epoch 74/500, Tr Loss: 0.4620, Tr Acc: 91.5000, Val Loss: 0.5520, Val Acc: 76.3333\n",
      "\n",
      "Starting epoch 75 / 500\n",
      "Epoch 75/500, Tr Loss: 0.4383, Tr Acc: 94.3333, Val Loss: 0.5349, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 76 / 500\n",
      "Epoch 76/500, Tr Loss: 0.4510, Tr Acc: 91.8333, Val Loss: 0.5407, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 77 / 500\n",
      "Epoch 77/500, Tr Loss: 0.4337, Tr Acc: 94.1667, Val Loss: 0.5484, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 78 / 500\n",
      "Epoch 78/500, Tr Loss: 0.4313, Tr Acc: 94.5000, Val Loss: 0.5350, Val Acc: 79.6667\n",
      "\n",
      "Starting epoch 79 / 500\n",
      "Epoch 79/500, Tr Loss: 0.4406, Tr Acc: 92.6667, Val Loss: 0.5369, Val Acc: 78.3333\n",
      "\n",
      "Starting epoch 80 / 500\n",
      "Epoch 80/500, Tr Loss: 0.4306, Tr Acc: 94.3333, Val Loss: 0.5386, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 81 / 500\n",
      "Epoch 81/500, Tr Loss: 0.4313, Tr Acc: 94.3333, Val Loss: 0.5285, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 82 / 500\n",
      "Epoch 82/500, Tr Loss: 0.4288, Tr Acc: 95.1667, Val Loss: 0.5314, Val Acc: 79.3333\n",
      "\n",
      "Starting epoch 83 / 500\n",
      "Epoch 83/500, Tr Loss: 0.4339, Tr Acc: 94.1667, Val Loss: 0.5208, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 84 / 500\n",
      "Epoch 84/500, Tr Loss: 0.4260, Tr Acc: 96.0000, Val Loss: 0.5397, Val Acc: 77.0000\n",
      "\n",
      "Starting epoch 85 / 500\n",
      "Epoch 85/500, Tr Loss: 0.4243, Tr Acc: 95.5000, Val Loss: 0.5342, Val Acc: 78.6667\n",
      "\n",
      "Starting epoch 86 / 500\n",
      "Epoch 86/500, Tr Loss: 0.4301, Tr Acc: 94.3333, Val Loss: 0.5178, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 87 / 500\n",
      "Epoch 87/500, Tr Loss: 0.4219, Tr Acc: 95.6667, Val Loss: 0.5285, Val Acc: 78.0000\n",
      "\n",
      "Starting epoch 88 / 500\n",
      "Epoch 88/500, Tr Loss: 0.4246, Tr Acc: 94.1667, Val Loss: 0.5264, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 89 / 500\n",
      "Epoch 89/500, Tr Loss: 0.4241, Tr Acc: 94.8333, Val Loss: 0.5271, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 90 / 500\n",
      "Epoch 90/500, Tr Loss: 0.4221, Tr Acc: 95.1667, Val Loss: 0.5330, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 91 / 500\n",
      "Epoch 91/500, Tr Loss: 0.4223, Tr Acc: 94.8333, Val Loss: 0.5241, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 92 / 500\n",
      "Epoch 92/500, Tr Loss: 0.4176, Tr Acc: 95.8333, Val Loss: 0.5227, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 93 / 500\n",
      "Epoch 93/500, Tr Loss: 0.4244, Tr Acc: 94.5000, Val Loss: 0.5211, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 94 / 500\n",
      "Epoch 94/500, Tr Loss: 0.4165, Tr Acc: 95.1667, Val Loss: 0.5218, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 95 / 500\n",
      "Epoch 95/500, Tr Loss: 0.4077, Tr Acc: 97.3333, Val Loss: 0.5310, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 96 / 500\n",
      "Epoch 96/500, Tr Loss: 0.4072, Tr Acc: 97.0000, Val Loss: 0.5354, Val Acc: 78.3333\n",
      "\n",
      "Starting epoch 97 / 500\n",
      "Epoch 97/500, Tr Loss: 0.4080, Tr Acc: 95.8333, Val Loss: 0.5189, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 98 / 500\n",
      "Epoch 98/500, Tr Loss: 0.4101, Tr Acc: 96.6667, Val Loss: 0.5101, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 99 / 500\n",
      "Epoch 99/500, Tr Loss: 0.4000, Tr Acc: 97.8333, Val Loss: 0.5261, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 100 / 500\n",
      "Epoch 100/500, Tr Loss: 0.4095, Tr Acc: 96.0000, Val Loss: 0.5319, Val Acc: 78.3333\n",
      "\n",
      "Starting epoch 101 / 500\n",
      "Epoch 101/500, Tr Loss: 0.3991, Tr Acc: 97.5000, Val Loss: 0.5138, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 102 / 500\n",
      "Epoch 102/500, Tr Loss: 0.4069, Tr Acc: 96.3333, Val Loss: 0.5316, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 103 / 500\n",
      "Epoch 103/500, Tr Loss: 0.3976, Tr Acc: 97.5000, Val Loss: 0.5299, Val Acc: 79.3333\n",
      "\n",
      "Starting epoch 104 / 500\n",
      "Epoch 104/500, Tr Loss: 0.3964, Tr Acc: 98.0000, Val Loss: 0.5175, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 105 / 500\n",
      "Epoch 105/500, Tr Loss: 0.4080, Tr Acc: 96.3333, Val Loss: 0.5125, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 106 / 500\n",
      "Epoch 106/500, Tr Loss: 0.4034, Tr Acc: 96.6667, Val Loss: 0.5220, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 107 / 500\n",
      "Epoch 107/500, Tr Loss: 0.4032, Tr Acc: 96.0000, Val Loss: 0.4994, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 108 / 500\n",
      "Epoch 108/500, Tr Loss: 0.3944, Tr Acc: 97.6667, Val Loss: 0.5086, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 109 / 500\n",
      "Epoch 109/500, Tr Loss: 0.3946, Tr Acc: 96.8333, Val Loss: 0.5316, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 110 / 500\n",
      "Epoch 110/500, Tr Loss: 0.3966, Tr Acc: 97.0000, Val Loss: 0.5345, Val Acc: 77.3333\n",
      "\n",
      "Starting epoch 111 / 500\n",
      "Epoch 111/500, Tr Loss: 0.3895, Tr Acc: 97.8333, Val Loss: 0.5216, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 112 / 500\n",
      "Epoch 112/500, Tr Loss: 0.3940, Tr Acc: 97.3333, Val Loss: 0.5140, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 113 / 500\n",
      "Epoch 113/500, Tr Loss: 0.3848, Tr Acc: 98.8333, Val Loss: 0.5041, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 114 / 500\n",
      "Epoch 114/500, Tr Loss: 0.3858, Tr Acc: 98.6667, Val Loss: 0.5162, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 115 / 500\n",
      "Epoch 115/500, Tr Loss: 0.3966, Tr Acc: 96.1667, Val Loss: 0.5126, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 116 / 500\n",
      "Epoch 116/500, Tr Loss: 0.3891, Tr Acc: 98.0000, Val Loss: 0.5215, Val Acc: 79.3333\n",
      "\n",
      "Starting epoch 117 / 500\n",
      "Epoch 117/500, Tr Loss: 0.3833, Tr Acc: 98.5000, Val Loss: 0.5147, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 118 / 500\n",
      "Epoch 118/500, Tr Loss: 0.3815, Tr Acc: 98.1667, Val Loss: 0.5184, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 119 / 500\n",
      "Epoch 119/500, Tr Loss: 0.3829, Tr Acc: 98.1667, Val Loss: 0.5056, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 120 / 500\n",
      "Epoch 120/500, Tr Loss: 0.3820, Tr Acc: 98.0000, Val Loss: 0.5072, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 121 / 500\n",
      "Epoch 121/500, Tr Loss: 0.3853, Tr Acc: 97.5000, Val Loss: 0.5162, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 122 / 500\n",
      "Epoch 122/500, Tr Loss: 0.3892, Tr Acc: 96.8333, Val Loss: 0.5028, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 123 / 500\n",
      "Epoch 123/500, Tr Loss: 0.3838, Tr Acc: 98.5000, Val Loss: 0.5296, Val Acc: 77.6667\n",
      "\n",
      "Starting epoch 124 / 500\n",
      "Epoch 124/500, Tr Loss: 0.3890, Tr Acc: 97.8333, Val Loss: 0.5228, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 125 / 500\n",
      "Epoch 125/500, Tr Loss: 0.3822, Tr Acc: 98.0000, Val Loss: 0.5233, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 126 / 500\n",
      "Epoch 126/500, Tr Loss: 0.3870, Tr Acc: 97.1667, Val Loss: 0.5146, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 127 / 500\n",
      "Epoch 127/500, Tr Loss: 0.3830, Tr Acc: 97.3333, Val Loss: 0.5043, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 128 / 500\n",
      "Epoch 128/500, Tr Loss: 0.3834, Tr Acc: 97.8333, Val Loss: 0.5094, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 129 / 500\n",
      "Epoch 129/500, Tr Loss: 0.3735, Tr Acc: 98.3333, Val Loss: 0.5096, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 130 / 500\n",
      "Epoch 130/500, Tr Loss: 0.3750, Tr Acc: 98.3333, Val Loss: 0.5093, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 131 / 500\n",
      "Epoch 131/500, Tr Loss: 0.3751, Tr Acc: 98.8333, Val Loss: 0.5096, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 132 / 500\n",
      "Epoch 132/500, Tr Loss: 0.3818, Tr Acc: 97.5000, Val Loss: 0.5115, Val Acc: 79.3333\n",
      "\n",
      "Starting epoch 133 / 500\n",
      "Epoch 133/500, Tr Loss: 0.3711, Tr Acc: 98.8333, Val Loss: 0.5083, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 134 / 500\n",
      "Epoch 134/500, Tr Loss: 0.3781, Tr Acc: 98.1667, Val Loss: 0.4960, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 135 / 500\n",
      "Epoch 135/500, Tr Loss: 0.3700, Tr Acc: 98.5000, Val Loss: 0.5121, Val Acc: 79.6667\n",
      "\n",
      "Starting epoch 136 / 500\n",
      "Epoch 136/500, Tr Loss: 0.3728, Tr Acc: 99.1667, Val Loss: 0.5149, Val Acc: 78.6667\n",
      "\n",
      "Starting epoch 137 / 500\n",
      "Epoch 137/500, Tr Loss: 0.3698, Tr Acc: 98.6667, Val Loss: 0.5281, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 138 / 500\n",
      "Epoch 138/500, Tr Loss: 0.3691, Tr Acc: 98.3333, Val Loss: 0.5095, Val Acc: 80.0000\n",
      "\n",
      "Starting epoch 139 / 500\n",
      "Epoch 139/500, Tr Loss: 0.3697, Tr Acc: 98.3333, Val Loss: 0.4970, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 140 / 500\n",
      "Epoch 140/500, Tr Loss: 0.3715, Tr Acc: 98.8333, Val Loss: 0.5007, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 141 / 500\n",
      "Epoch 141/500, Tr Loss: 0.3717, Tr Acc: 98.3333, Val Loss: 0.5053, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 142 / 500\n",
      "Epoch 142/500, Tr Loss: 0.3705, Tr Acc: 98.6667, Val Loss: 0.4958, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 143 / 500\n",
      "Epoch 143/500, Tr Loss: 0.3701, Tr Acc: 98.6667, Val Loss: 0.5097, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 144 / 500\n",
      "Epoch 144/500, Tr Loss: 0.3708, Tr Acc: 99.1667, Val Loss: 0.5040, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 145 / 500\n",
      "Epoch 145/500, Tr Loss: 0.3679, Tr Acc: 98.8333, Val Loss: 0.5058, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 146 / 500\n",
      "Epoch 146/500, Tr Loss: 0.3673, Tr Acc: 99.1667, Val Loss: 0.5037, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 147 / 500\n",
      "Epoch 147/500, Tr Loss: 0.3665, Tr Acc: 99.3333, Val Loss: 0.4971, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 148 / 500\n",
      "Epoch 148/500, Tr Loss: 0.3634, Tr Acc: 99.5000, Val Loss: 0.5112, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 149 / 500\n",
      "Epoch 149/500, Tr Loss: 0.3642, Tr Acc: 98.8333, Val Loss: 0.5114, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 150 / 500\n",
      "Epoch 150/500, Tr Loss: 0.3604, Tr Acc: 98.6667, Val Loss: 0.5017, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 151 / 500\n",
      "Epoch 151/500, Tr Loss: 0.3618, Tr Acc: 99.3333, Val Loss: 0.4984, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 152 / 500\n",
      "Epoch 152/500, Tr Loss: 0.3587, Tr Acc: 99.3333, Val Loss: 0.5031, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 153 / 500\n",
      "Epoch 153/500, Tr Loss: 0.3686, Tr Acc: 99.0000, Val Loss: 0.4950, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 154 / 500\n",
      "Epoch 154/500, Tr Loss: 0.3617, Tr Acc: 99.1667, Val Loss: 0.4993, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 155 / 500\n",
      "Epoch 155/500, Tr Loss: 0.3638, Tr Acc: 99.3333, Val Loss: 0.5009, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 156 / 500\n",
      "Epoch 156/500, Tr Loss: 0.3671, Tr Acc: 98.5000, Val Loss: 0.4911, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 157 / 500\n",
      "Epoch 157/500, Tr Loss: 0.3605, Tr Acc: 99.5000, Val Loss: 0.4953, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 158 / 500\n",
      "Epoch 158/500, Tr Loss: 0.3648, Tr Acc: 99.0000, Val Loss: 0.5022, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 159 / 500\n",
      "Epoch 159/500, Tr Loss: 0.3657, Tr Acc: 98.5000, Val Loss: 0.5114, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 160 / 500\n",
      "Epoch 160/500, Tr Loss: 0.3627, Tr Acc: 98.8333, Val Loss: 0.5089, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 161 / 500\n",
      "Epoch 161/500, Tr Loss: 0.3581, Tr Acc: 99.1667, Val Loss: 0.4956, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 162 / 500\n",
      "Epoch 162/500, Tr Loss: 0.3648, Tr Acc: 98.5000, Val Loss: 0.4984, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 163 / 500\n",
      "Epoch 163/500, Tr Loss: 0.3586, Tr Acc: 99.1667, Val Loss: 0.4944, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 164 / 500\n",
      "Epoch 164/500, Tr Loss: 0.3673, Tr Acc: 98.3333, Val Loss: 0.4977, Val Acc: 80.3333\n",
      "\n",
      "Starting epoch 165 / 500\n",
      "Epoch 165/500, Tr Loss: 0.3583, Tr Acc: 98.8333, Val Loss: 0.4973, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 166 / 500\n",
      "Epoch 166/500, Tr Loss: 0.3598, Tr Acc: 99.1667, Val Loss: 0.4918, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 167 / 500\n",
      "Epoch 167/500, Tr Loss: 0.3580, Tr Acc: 99.1667, Val Loss: 0.5025, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 168 / 500\n",
      "Epoch 168/500, Tr Loss: 0.3548, Tr Acc: 99.1667, Val Loss: 0.4923, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 169 / 500\n",
      "Epoch 169/500, Tr Loss: 0.3618, Tr Acc: 99.0000, Val Loss: 0.4975, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 170 / 500\n",
      "Epoch 170/500, Tr Loss: 0.3565, Tr Acc: 99.1667, Val Loss: 0.4955, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 171 / 500\n",
      "Epoch 171/500, Tr Loss: 0.3573, Tr Acc: 98.8333, Val Loss: 0.4918, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 172 / 500\n",
      "Epoch 172/500, Tr Loss: 0.3630, Tr Acc: 98.8333, Val Loss: 0.4887, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 173 / 500\n",
      "Epoch 173/500, Tr Loss: 0.3608, Tr Acc: 99.0000, Val Loss: 0.5136, Val Acc: 79.0000\n",
      "\n",
      "Starting epoch 174 / 500\n",
      "Epoch 174/500, Tr Loss: 0.3519, Tr Acc: 99.3333, Val Loss: 0.5000, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 175 / 500\n",
      "Epoch 175/500, Tr Loss: 0.3587, Tr Acc: 98.6667, Val Loss: 0.4945, Val Acc: 80.6667\n",
      "\n",
      "Starting epoch 176 / 500\n",
      "Epoch 176/500, Tr Loss: 0.3635, Tr Acc: 97.8333, Val Loss: 0.4907, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 177 / 500\n",
      "Epoch 177/500, Tr Loss: 0.3572, Tr Acc: 99.3333, Val Loss: 0.4994, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 178 / 500\n",
      "Epoch 178/500, Tr Loss: 0.3556, Tr Acc: 99.3333, Val Loss: 0.5024, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 179 / 500\n",
      "Epoch 179/500, Tr Loss: 0.3549, Tr Acc: 98.8333, Val Loss: 0.4983, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 180 / 500\n",
      "Epoch 180/500, Tr Loss: 0.3509, Tr Acc: 99.5000, Val Loss: 0.4900, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 181 / 500\n",
      "Epoch 181/500, Tr Loss: 0.3557, Tr Acc: 99.0000, Val Loss: 0.4949, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 182 / 500\n",
      "Epoch 182/500, Tr Loss: 0.3564, Tr Acc: 99.8333, Val Loss: 0.4958, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 183 / 500\n",
      "Epoch 183/500, Tr Loss: 0.3476, Tr Acc: 99.3333, Val Loss: 0.4947, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 184 / 500\n",
      "Epoch 184/500, Tr Loss: 0.3464, Tr Acc: 99.6667, Val Loss: 0.5115, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 185 / 500\n",
      "Epoch 185/500, Tr Loss: 0.3494, Tr Acc: 99.8333, Val Loss: 0.5023, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 186 / 500\n",
      "Epoch 186/500, Tr Loss: 0.3479, Tr Acc: 99.5000, Val Loss: 0.4906, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 187 / 500\n",
      "Epoch 187/500, Tr Loss: 0.3521, Tr Acc: 99.3333, Val Loss: 0.4838, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 188 / 500\n",
      "Epoch 188/500, Tr Loss: 0.3489, Tr Acc: 99.1667, Val Loss: 0.5046, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 189 / 500\n",
      "Epoch 189/500, Tr Loss: 0.3573, Tr Acc: 98.8333, Val Loss: 0.5020, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 190 / 500\n",
      "Epoch 190/500, Tr Loss: 0.3613, Tr Acc: 98.8333, Val Loss: 0.4986, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 191 / 500\n",
      "Epoch 191/500, Tr Loss: 0.3462, Tr Acc: 99.5000, Val Loss: 0.4997, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 192 / 500\n",
      "Epoch 192/500, Tr Loss: 0.3495, Tr Acc: 99.8333, Val Loss: 0.4988, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 193 / 500\n",
      "Epoch 193/500, Tr Loss: 0.3503, Tr Acc: 99.0000, Val Loss: 0.4990, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 194 / 500\n",
      "Epoch 194/500, Tr Loss: 0.3521, Tr Acc: 99.1667, Val Loss: 0.4958, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 195 / 500\n",
      "Epoch 195/500, Tr Loss: 0.3494, Tr Acc: 99.3333, Val Loss: 0.4929, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 196 / 500\n",
      "Epoch 196/500, Tr Loss: 0.3504, Tr Acc: 99.3333, Val Loss: 0.4863, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 197 / 500\n",
      "Epoch 197/500, Tr Loss: 0.3523, Tr Acc: 99.3333, Val Loss: 0.4959, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 198 / 500\n",
      "Epoch 198/500, Tr Loss: 0.3478, Tr Acc: 99.5000, Val Loss: 0.5050, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 199 / 500\n",
      "Epoch 199/500, Tr Loss: 0.3484, Tr Acc: 99.5000, Val Loss: 0.4783, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 200 / 500\n",
      "Epoch 200/500, Tr Loss: 0.3604, Tr Acc: 98.5000, Val Loss: 0.5047, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 201 / 500\n",
      "Epoch 201/500, Tr Loss: 0.3505, Tr Acc: 99.3333, Val Loss: 0.4947, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 202 / 500\n",
      "Epoch 202/500, Tr Loss: 0.3476, Tr Acc: 99.5000, Val Loss: 0.4953, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 203 / 500\n",
      "Epoch 203/500, Tr Loss: 0.3451, Tr Acc: 99.5000, Val Loss: 0.4820, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 204 / 500\n",
      "Epoch 204/500, Tr Loss: 0.3409, Tr Acc: 99.8333, Val Loss: 0.4928, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 205 / 500\n",
      "Epoch 205/500, Tr Loss: 0.3519, Tr Acc: 99.1667, Val Loss: 0.4920, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 206 / 500\n",
      "Epoch 206/500, Tr Loss: 0.3405, Tr Acc: 99.8333, Val Loss: 0.5014, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 207 / 500\n",
      "Epoch 207/500, Tr Loss: 0.3437, Tr Acc: 99.6667, Val Loss: 0.4895, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 208 / 500\n",
      "Epoch 208/500, Tr Loss: 0.3479, Tr Acc: 99.1667, Val Loss: 0.4808, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 209 / 500\n",
      "Epoch 209/500, Tr Loss: 0.3477, Tr Acc: 99.1667, Val Loss: 0.4903, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 210 / 500\n",
      "Epoch 210/500, Tr Loss: 0.3419, Tr Acc: 99.8333, Val Loss: 0.4820, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 211 / 500\n",
      "Epoch 211/500, Tr Loss: 0.3472, Tr Acc: 99.3333, Val Loss: 0.4901, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 212 / 500\n",
      "Epoch 212/500, Tr Loss: 0.3442, Tr Acc: 99.6667, Val Loss: 0.4913, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 213 / 500\n",
      "Epoch 213/500, Tr Loss: 0.3416, Tr Acc: 99.6667, Val Loss: 0.4856, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 214 / 500\n",
      "Epoch 214/500, Tr Loss: 0.3447, Tr Acc: 99.6667, Val Loss: 0.4808, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 215 / 500\n",
      "Epoch 215/500, Tr Loss: 0.3483, Tr Acc: 99.1667, Val Loss: 0.4966, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 216 / 500\n",
      "Epoch 216/500, Tr Loss: 0.3426, Tr Acc: 99.6667, Val Loss: 0.4814, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 217 / 500\n",
      "Epoch 217/500, Tr Loss: 0.3485, Tr Acc: 99.5000, Val Loss: 0.4775, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 218 / 500\n",
      "Epoch 218/500, Tr Loss: 0.3472, Tr Acc: 99.6667, Val Loss: 0.4972, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 219 / 500\n",
      "Epoch 219/500, Tr Loss: 0.3552, Tr Acc: 98.1667, Val Loss: 0.4797, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 220 / 500\n",
      "Epoch 220/500, Tr Loss: 0.3404, Tr Acc: 99.8333, Val Loss: 0.4866, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 221 / 500\n",
      "Epoch 221/500, Tr Loss: 0.3410, Tr Acc: 99.5000, Val Loss: 0.4877, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 222 / 500\n",
      "Epoch 222/500, Tr Loss: 0.3482, Tr Acc: 99.6667, Val Loss: 0.4780, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 223 / 500\n",
      "Epoch 223/500, Tr Loss: 0.3459, Tr Acc: 99.5000, Val Loss: 0.4954, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 224 / 500\n",
      "Epoch 224/500, Tr Loss: 0.3502, Tr Acc: 98.6667, Val Loss: 0.4762, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 225 / 500\n",
      "Epoch 225/500, Tr Loss: 0.3437, Tr Acc: 99.3333, Val Loss: 0.4827, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 226 / 500\n",
      "Epoch 226/500, Tr Loss: 0.3384, Tr Acc: 99.6667, Val Loss: 0.4800, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 227 / 500\n",
      "Epoch 227/500, Tr Loss: 0.3397, Tr Acc: 100.0000, Val Loss: 0.4916, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 228 / 500\n",
      "Epoch 228/500, Tr Loss: 0.3412, Tr Acc: 99.6667, Val Loss: 0.4794, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 229 / 500\n",
      "Epoch 229/500, Tr Loss: 0.3490, Tr Acc: 99.5000, Val Loss: 0.4903, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 230 / 500\n",
      "Epoch 230/500, Tr Loss: 0.3421, Tr Acc: 99.3333, Val Loss: 0.4786, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 231 / 500\n",
      "Epoch 231/500, Tr Loss: 0.3373, Tr Acc: 99.8333, Val Loss: 0.4953, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 232 / 500\n",
      "Epoch 232/500, Tr Loss: 0.3403, Tr Acc: 99.6667, Val Loss: 0.4889, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 233 / 500\n",
      "Epoch 233/500, Tr Loss: 0.3365, Tr Acc: 99.8333, Val Loss: 0.4885, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 234 / 500\n",
      "Epoch 234/500, Tr Loss: 0.3377, Tr Acc: 100.0000, Val Loss: 0.4842, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 235 / 500\n",
      "Epoch 235/500, Tr Loss: 0.3409, Tr Acc: 99.8333, Val Loss: 0.4865, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 236 / 500\n",
      "Epoch 236/500, Tr Loss: 0.3431, Tr Acc: 99.0000, Val Loss: 0.4894, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 237 / 500\n",
      "Epoch 237/500, Tr Loss: 0.3445, Tr Acc: 99.3333, Val Loss: 0.4855, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 238 / 500\n",
      "Epoch 238/500, Tr Loss: 0.3400, Tr Acc: 99.6667, Val Loss: 0.4940, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 239 / 500\n",
      "Epoch 239/500, Tr Loss: 0.3397, Tr Acc: 99.8333, Val Loss: 0.4802, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 240 / 500\n",
      "Epoch 240/500, Tr Loss: 0.3432, Tr Acc: 99.8333, Val Loss: 0.4927, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 241 / 500\n",
      "Epoch 241/500, Tr Loss: 0.3364, Tr Acc: 99.8333, Val Loss: 0.4843, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 242 / 500\n",
      "Epoch 242/500, Tr Loss: 0.3397, Tr Acc: 99.1667, Val Loss: 0.4879, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 243 / 500\n",
      "Epoch 243/500, Tr Loss: 0.3364, Tr Acc: 99.8333, Val Loss: 0.4817, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 244 / 500\n",
      "Epoch 244/500, Tr Loss: 0.3374, Tr Acc: 99.6667, Val Loss: 0.4809, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 245 / 500\n",
      "Epoch 245/500, Tr Loss: 0.3396, Tr Acc: 99.3333, Val Loss: 0.4880, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 246 / 500\n",
      "Epoch 246/500, Tr Loss: 0.3386, Tr Acc: 100.0000, Val Loss: 0.4857, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 247 / 500\n",
      "Epoch 247/500, Tr Loss: 0.3376, Tr Acc: 99.8333, Val Loss: 0.4854, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 248 / 500\n",
      "Epoch 248/500, Tr Loss: 0.3372, Tr Acc: 99.5000, Val Loss: 0.4876, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 249 / 500\n",
      "Epoch 249/500, Tr Loss: 0.3396, Tr Acc: 100.0000, Val Loss: 0.4812, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 250 / 500\n",
      "Epoch 250/500, Tr Loss: 0.3364, Tr Acc: 99.8333, Val Loss: 0.4821, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 251 / 500\n",
      "Epoch 251/500, Tr Loss: 0.3389, Tr Acc: 99.3333, Val Loss: 0.4766, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 252 / 500\n",
      "Epoch 252/500, Tr Loss: 0.3428, Tr Acc: 99.1667, Val Loss: 0.4822, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 253 / 500\n",
      "Epoch 253/500, Tr Loss: 0.3382, Tr Acc: 99.6667, Val Loss: 0.4835, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 254 / 500\n",
      "Epoch 254/500, Tr Loss: 0.3423, Tr Acc: 99.5000, Val Loss: 0.4760, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 255 / 500\n",
      "Epoch 255/500, Tr Loss: 0.3364, Tr Acc: 99.8333, Val Loss: 0.4836, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 256 / 500\n",
      "Epoch 256/500, Tr Loss: 0.3391, Tr Acc: 99.6667, Val Loss: 0.4791, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 257 / 500\n",
      "Epoch 257/500, Tr Loss: 0.3352, Tr Acc: 99.6667, Val Loss: 0.4898, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 258 / 500\n",
      "Epoch 258/500, Tr Loss: 0.3349, Tr Acc: 99.8333, Val Loss: 0.4856, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 259 / 500\n",
      "Epoch 259/500, Tr Loss: 0.3358, Tr Acc: 100.0000, Val Loss: 0.4888, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 260 / 500\n",
      "Epoch 260/500, Tr Loss: 0.3409, Tr Acc: 99.5000, Val Loss: 0.4773, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 261 / 500\n",
      "Epoch 261/500, Tr Loss: 0.3356, Tr Acc: 99.6667, Val Loss: 0.4747, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 262 / 500\n",
      "Epoch 262/500, Tr Loss: 0.3338, Tr Acc: 99.8333, Val Loss: 0.4677, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 263 / 500\n",
      "Epoch 263/500, Tr Loss: 0.3378, Tr Acc: 99.5000, Val Loss: 0.4805, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 264 / 500\n",
      "Epoch 264/500, Tr Loss: 0.3447, Tr Acc: 99.5000, Val Loss: 0.4789, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 265 / 500\n",
      "Epoch 265/500, Tr Loss: 0.3443, Tr Acc: 98.6667, Val Loss: 0.4893, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 266 / 500\n",
      "Epoch 266/500, Tr Loss: 0.3383, Tr Acc: 99.3333, Val Loss: 0.4844, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 267 / 500\n",
      "Epoch 267/500, Tr Loss: 0.3467, Tr Acc: 99.0000, Val Loss: 0.4777, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 268 / 500\n",
      "Epoch 268/500, Tr Loss: 0.3330, Tr Acc: 99.8333, Val Loss: 0.4805, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 269 / 500\n",
      "Epoch 269/500, Tr Loss: 0.3392, Tr Acc: 99.5000, Val Loss: 0.4798, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 270 / 500\n",
      "Epoch 270/500, Tr Loss: 0.3342, Tr Acc: 99.8333, Val Loss: 0.4872, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 271 / 500\n",
      "Epoch 271/500, Tr Loss: 0.3338, Tr Acc: 99.8333, Val Loss: 0.4780, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 272 / 500\n",
      "Epoch 272/500, Tr Loss: 0.3384, Tr Acc: 99.3333, Val Loss: 0.4772, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 273 / 500\n",
      "Epoch 273/500, Tr Loss: 0.3348, Tr Acc: 99.8333, Val Loss: 0.4897, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 274 / 500\n",
      "Epoch 274/500, Tr Loss: 0.3362, Tr Acc: 100.0000, Val Loss: 0.4867, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 275 / 500\n",
      "Epoch 275/500, Tr Loss: 0.3353, Tr Acc: 99.5000, Val Loss: 0.4909, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 276 / 500\n",
      "Epoch 276/500, Tr Loss: 0.3358, Tr Acc: 99.8333, Val Loss: 0.4866, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 277 / 500\n",
      "Epoch 277/500, Tr Loss: 0.3364, Tr Acc: 99.6667, Val Loss: 0.4781, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 278 / 500\n",
      "Epoch 278/500, Tr Loss: 0.3397, Tr Acc: 99.6667, Val Loss: 0.4742, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 279 / 500\n",
      "Epoch 279/500, Tr Loss: 0.3332, Tr Acc: 99.8333, Val Loss: 0.4884, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 280 / 500\n",
      "Epoch 280/500, Tr Loss: 0.3330, Tr Acc: 99.8333, Val Loss: 0.4753, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 281 / 500\n",
      "Epoch 281/500, Tr Loss: 0.3341, Tr Acc: 99.5000, Val Loss: 0.4824, Val Acc: 84.0000\n",
      "\n",
      "Starting epoch 282 / 500\n",
      "Epoch 282/500, Tr Loss: 0.3408, Tr Acc: 99.6667, Val Loss: 0.4875, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 283 / 500\n",
      "Epoch 283/500, Tr Loss: 0.3349, Tr Acc: 99.8333, Val Loss: 0.4831, Val Acc: 84.0000\n",
      "\n",
      "Starting epoch 284 / 500\n",
      "Epoch 284/500, Tr Loss: 0.3339, Tr Acc: 99.8333, Val Loss: 0.4789, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 285 / 500\n",
      "Epoch 285/500, Tr Loss: 0.3332, Tr Acc: 100.0000, Val Loss: 0.4773, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 286 / 500\n",
      "Epoch 286/500, Tr Loss: 0.3327, Tr Acc: 100.0000, Val Loss: 0.4734, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 287 / 500\n",
      "Epoch 287/500, Tr Loss: 0.3323, Tr Acc: 100.0000, Val Loss: 0.4764, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 288 / 500\n",
      "Epoch 288/500, Tr Loss: 0.3321, Tr Acc: 100.0000, Val Loss: 0.4695, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 289 / 500\n",
      "Epoch 289/500, Tr Loss: 0.3363, Tr Acc: 99.6667, Val Loss: 0.4822, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 290 / 500\n",
      "Epoch 290/500, Tr Loss: 0.3330, Tr Acc: 100.0000, Val Loss: 0.4691, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 291 / 500\n",
      "Epoch 291/500, Tr Loss: 0.3363, Tr Acc: 99.6667, Val Loss: 0.4743, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 292 / 500\n",
      "Epoch 292/500, Tr Loss: 0.3320, Tr Acc: 100.0000, Val Loss: 0.4713, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 293 / 500\n",
      "Epoch 293/500, Tr Loss: 0.3343, Tr Acc: 99.8333, Val Loss: 0.4797, Val Acc: 84.3333\n",
      "\n",
      "Starting epoch 294 / 500\n",
      "Epoch 294/500, Tr Loss: 0.3306, Tr Acc: 100.0000, Val Loss: 0.4809, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 295 / 500\n",
      "Epoch 295/500, Tr Loss: 0.3385, Tr Acc: 99.5000, Val Loss: 0.4798, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 296 / 500\n",
      "Epoch 296/500, Tr Loss: 0.3330, Tr Acc: 99.6667, Val Loss: 0.4790, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 297 / 500\n",
      "Epoch 297/500, Tr Loss: 0.3332, Tr Acc: 99.6667, Val Loss: 0.4835, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 298 / 500\n",
      "Epoch 298/500, Tr Loss: 0.3387, Tr Acc: 99.0000, Val Loss: 0.4757, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 299 / 500\n",
      "Epoch 299/500, Tr Loss: 0.3339, Tr Acc: 99.8333, Val Loss: 0.4778, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 300 / 500\n",
      "Epoch 300/500, Tr Loss: 0.3395, Tr Acc: 99.1667, Val Loss: 0.4682, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 301 / 500\n",
      "Epoch 301/500, Tr Loss: 0.3309, Tr Acc: 100.0000, Val Loss: 0.4739, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 302 / 500\n",
      "Epoch 302/500, Tr Loss: 0.3309, Tr Acc: 99.8333, Val Loss: 0.4876, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 303 / 500\n",
      "Epoch 303/500, Tr Loss: 0.3361, Tr Acc: 99.5000, Val Loss: 0.4717, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 304 / 500\n",
      "Epoch 304/500, Tr Loss: 0.3316, Tr Acc: 99.8333, Val Loss: 0.4814, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 305 / 500\n",
      "Epoch 305/500, Tr Loss: 0.3357, Tr Acc: 99.3333, Val Loss: 0.4793, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 306 / 500\n",
      "Epoch 306/500, Tr Loss: 0.3310, Tr Acc: 99.6667, Val Loss: 0.4762, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 307 / 500\n",
      "Epoch 307/500, Tr Loss: 0.3350, Tr Acc: 99.6667, Val Loss: 0.4729, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 308 / 500\n",
      "Epoch 308/500, Tr Loss: 0.3316, Tr Acc: 99.8333, Val Loss: 0.4759, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 309 / 500\n",
      "Epoch 309/500, Tr Loss: 0.3329, Tr Acc: 99.5000, Val Loss: 0.4865, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 310 / 500\n",
      "Epoch 310/500, Tr Loss: 0.3341, Tr Acc: 99.5000, Val Loss: 0.4787, Val Acc: 84.0000\n",
      "\n",
      "Starting epoch 311 / 500\n",
      "Epoch 311/500, Tr Loss: 0.3346, Tr Acc: 99.5000, Val Loss: 0.4792, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 312 / 500\n",
      "Epoch 312/500, Tr Loss: 0.3299, Tr Acc: 99.8333, Val Loss: 0.4784, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 313 / 500\n",
      "Epoch 313/500, Tr Loss: 0.3345, Tr Acc: 99.6667, Val Loss: 0.4764, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 314 / 500\n",
      "Epoch 314/500, Tr Loss: 0.3354, Tr Acc: 99.6667, Val Loss: 0.4768, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 315 / 500\n",
      "Epoch 315/500, Tr Loss: 0.3349, Tr Acc: 100.0000, Val Loss: 0.4718, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 316 / 500\n",
      "Epoch 316/500, Tr Loss: 0.3300, Tr Acc: 99.6667, Val Loss: 0.4756, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 317 / 500\n",
      "Epoch 317/500, Tr Loss: 0.3332, Tr Acc: 99.5000, Val Loss: 0.4860, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 318 / 500\n",
      "Epoch 318/500, Tr Loss: 0.3293, Tr Acc: 99.8333, Val Loss: 0.4781, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 319 / 500\n",
      "Epoch 319/500, Tr Loss: 0.3309, Tr Acc: 100.0000, Val Loss: 0.4740, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 320 / 500\n",
      "Epoch 320/500, Tr Loss: 0.3298, Tr Acc: 100.0000, Val Loss: 0.4782, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 321 / 500\n",
      "Epoch 321/500, Tr Loss: 0.3304, Tr Acc: 100.0000, Val Loss: 0.4701, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 322 / 500\n",
      "Epoch 322/500, Tr Loss: 0.3273, Tr Acc: 100.0000, Val Loss: 0.4849, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 323 / 500\n",
      "Epoch 323/500, Tr Loss: 0.3290, Tr Acc: 100.0000, Val Loss: 0.4682, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 324 / 500\n",
      "Epoch 324/500, Tr Loss: 0.3268, Tr Acc: 100.0000, Val Loss: 0.4707, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 325 / 500\n",
      "Epoch 325/500, Tr Loss: 0.3327, Tr Acc: 99.6667, Val Loss: 0.4724, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 326 / 500\n",
      "Epoch 326/500, Tr Loss: 0.3368, Tr Acc: 99.1667, Val Loss: 0.4862, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 327 / 500\n",
      "Epoch 327/500, Tr Loss: 0.3282, Tr Acc: 100.0000, Val Loss: 0.4786, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 328 / 500\n",
      "Epoch 328/500, Tr Loss: 0.3294, Tr Acc: 99.8333, Val Loss: 0.4765, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 329 / 500\n",
      "Epoch 329/500, Tr Loss: 0.3312, Tr Acc: 100.0000, Val Loss: 0.4718, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 330 / 500\n",
      "Epoch 330/500, Tr Loss: 0.3303, Tr Acc: 100.0000, Val Loss: 0.4724, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 331 / 500\n",
      "Epoch 331/500, Tr Loss: 0.3319, Tr Acc: 99.8333, Val Loss: 0.4761, Val Acc: 84.3333\n",
      "\n",
      "Starting epoch 332 / 500\n",
      "Epoch 332/500, Tr Loss: 0.3332, Tr Acc: 99.6667, Val Loss: 0.4695, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 333 / 500\n",
      "Epoch 333/500, Tr Loss: 0.3271, Tr Acc: 99.6667, Val Loss: 0.4860, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 334 / 500\n",
      "Epoch 334/500, Tr Loss: 0.3278, Tr Acc: 100.0000, Val Loss: 0.4818, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 335 / 500\n",
      "Epoch 335/500, Tr Loss: 0.3340, Tr Acc: 99.5000, Val Loss: 0.4841, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 336 / 500\n",
      "Epoch 336/500, Tr Loss: 0.3313, Tr Acc: 99.6667, Val Loss: 0.4772, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 337 / 500\n",
      "Epoch 337/500, Tr Loss: 0.3287, Tr Acc: 100.0000, Val Loss: 0.4849, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 338 / 500\n",
      "Epoch 338/500, Tr Loss: 0.3288, Tr Acc: 100.0000, Val Loss: 0.4737, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 339 / 500\n",
      "Epoch 339/500, Tr Loss: 0.3318, Tr Acc: 100.0000, Val Loss: 0.4807, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 340 / 500\n",
      "Epoch 340/500, Tr Loss: 0.3281, Tr Acc: 99.8333, Val Loss: 0.4768, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 341 / 500\n",
      "Epoch 341/500, Tr Loss: 0.3283, Tr Acc: 99.8333, Val Loss: 0.4736, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 342 / 500\n",
      "Epoch 342/500, Tr Loss: 0.3287, Tr Acc: 100.0000, Val Loss: 0.4892, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 343 / 500\n",
      "Epoch 343/500, Tr Loss: 0.3381, Tr Acc: 98.8333, Val Loss: 0.4724, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 344 / 500\n",
      "Epoch 344/500, Tr Loss: 0.3307, Tr Acc: 99.8333, Val Loss: 0.4680, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 345 / 500\n",
      "Epoch 345/500, Tr Loss: 0.3298, Tr Acc: 99.8333, Val Loss: 0.4776, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 346 / 500\n",
      "Epoch 346/500, Tr Loss: 0.3303, Tr Acc: 99.6667, Val Loss: 0.4848, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 347 / 500\n",
      "Epoch 347/500, Tr Loss: 0.3263, Tr Acc: 100.0000, Val Loss: 0.4828, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 348 / 500\n",
      "Epoch 348/500, Tr Loss: 0.3282, Tr Acc: 99.8333, Val Loss: 0.4733, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 349 / 500\n",
      "Epoch 349/500, Tr Loss: 0.3331, Tr Acc: 99.8333, Val Loss: 0.4985, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 350 / 500\n",
      "Epoch 350/500, Tr Loss: 0.3268, Tr Acc: 99.8333, Val Loss: 0.4729, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 351 / 500\n",
      "Epoch 351/500, Tr Loss: 0.3289, Tr Acc: 99.8333, Val Loss: 0.4815, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 352 / 500\n",
      "Epoch 352/500, Tr Loss: 0.3361, Tr Acc: 99.1667, Val Loss: 0.4772, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 353 / 500\n",
      "Epoch 353/500, Tr Loss: 0.3299, Tr Acc: 99.8333, Val Loss: 0.4805, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 354 / 500\n",
      "Epoch 354/500, Tr Loss: 0.3293, Tr Acc: 99.6667, Val Loss: 0.4766, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 355 / 500\n",
      "Epoch 355/500, Tr Loss: 0.3292, Tr Acc: 100.0000, Val Loss: 0.4814, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 356 / 500\n",
      "Epoch 356/500, Tr Loss: 0.3297, Tr Acc: 99.8333, Val Loss: 0.4825, Val Acc: 84.0000\n",
      "\n",
      "Starting epoch 357 / 500\n",
      "Epoch 357/500, Tr Loss: 0.3323, Tr Acc: 99.3333, Val Loss: 0.4725, Val Acc: 85.3333\n",
      "\n",
      "Starting epoch 358 / 500\n",
      "Epoch 358/500, Tr Loss: 0.3270, Tr Acc: 99.8333, Val Loss: 0.4710, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 359 / 500\n",
      "Epoch 359/500, Tr Loss: 0.3287, Tr Acc: 99.8333, Val Loss: 0.4677, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 360 / 500\n",
      "Epoch 360/500, Tr Loss: 0.3358, Tr Acc: 99.0000, Val Loss: 0.4796, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 361 / 500\n",
      "Epoch 361/500, Tr Loss: 0.3299, Tr Acc: 99.5000, Val Loss: 0.4811, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 362 / 500\n",
      "Epoch 362/500, Tr Loss: 0.3327, Tr Acc: 99.1667, Val Loss: 0.4785, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 363 / 500\n",
      "Epoch 363/500, Tr Loss: 0.3330, Tr Acc: 99.8333, Val Loss: 0.4834, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 364 / 500\n",
      "Epoch 364/500, Tr Loss: 0.3348, Tr Acc: 99.6667, Val Loss: 0.4680, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 365 / 500\n",
      "Epoch 365/500, Tr Loss: 0.3287, Tr Acc: 100.0000, Val Loss: 0.4700, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 366 / 500\n",
      "Epoch 366/500, Tr Loss: 0.3257, Tr Acc: 100.0000, Val Loss: 0.4738, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 367 / 500\n",
      "Epoch 367/500, Tr Loss: 0.3326, Tr Acc: 99.3333, Val Loss: 0.4801, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 368 / 500\n",
      "Epoch 368/500, Tr Loss: 0.3355, Tr Acc: 99.3333, Val Loss: 0.4902, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 369 / 500\n",
      "Epoch 369/500, Tr Loss: 0.3307, Tr Acc: 99.8333, Val Loss: 0.4816, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 370 / 500\n",
      "Epoch 370/500, Tr Loss: 0.3266, Tr Acc: 100.0000, Val Loss: 0.4812, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 371 / 500\n",
      "Epoch 371/500, Tr Loss: 0.3364, Tr Acc: 99.5000, Val Loss: 0.4834, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 372 / 500\n",
      "Epoch 372/500, Tr Loss: 0.3343, Tr Acc: 99.5000, Val Loss: 0.4725, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 373 / 500\n",
      "Epoch 373/500, Tr Loss: 0.3303, Tr Acc: 99.6667, Val Loss: 0.4795, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 374 / 500\n",
      "Epoch 374/500, Tr Loss: 0.3283, Tr Acc: 100.0000, Val Loss: 0.4773, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 375 / 500\n",
      "Epoch 375/500, Tr Loss: 0.3294, Tr Acc: 99.6667, Val Loss: 0.4771, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 376 / 500\n",
      "Epoch 376/500, Tr Loss: 0.3278, Tr Acc: 99.6667, Val Loss: 0.4708, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 377 / 500\n",
      "Epoch 377/500, Tr Loss: 0.3275, Tr Acc: 99.8333, Val Loss: 0.4837, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 378 / 500\n",
      "Epoch 378/500, Tr Loss: 0.3282, Tr Acc: 100.0000, Val Loss: 0.4862, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 379 / 500\n",
      "Epoch 379/500, Tr Loss: 0.3358, Tr Acc: 99.0000, Val Loss: 0.4764, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 380 / 500\n",
      "Epoch 380/500, Tr Loss: 0.3341, Tr Acc: 99.1667, Val Loss: 0.4780, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 381 / 500\n",
      "Epoch 381/500, Tr Loss: 0.3310, Tr Acc: 99.8333, Val Loss: 0.4758, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 382 / 500\n",
      "Epoch 382/500, Tr Loss: 0.3268, Tr Acc: 99.8333, Val Loss: 0.4775, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 383 / 500\n",
      "Epoch 383/500, Tr Loss: 0.3270, Tr Acc: 100.0000, Val Loss: 0.4926, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 384 / 500\n",
      "Epoch 384/500, Tr Loss: 0.3301, Tr Acc: 100.0000, Val Loss: 0.4884, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 385 / 500\n",
      "Epoch 385/500, Tr Loss: 0.3242, Tr Acc: 100.0000, Val Loss: 0.4748, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 386 / 500\n",
      "Epoch 386/500, Tr Loss: 0.3288, Tr Acc: 99.5000, Val Loss: 0.4937, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 387 / 500\n",
      "Epoch 387/500, Tr Loss: 0.3258, Tr Acc: 99.8333, Val Loss: 0.4794, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 388 / 500\n",
      "Epoch 388/500, Tr Loss: 0.3274, Tr Acc: 99.6667, Val Loss: 0.4780, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 389 / 500\n",
      "Epoch 389/500, Tr Loss: 0.3299, Tr Acc: 99.6667, Val Loss: 0.4674, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 390 / 500\n",
      "Epoch 390/500, Tr Loss: 0.3284, Tr Acc: 99.8333, Val Loss: 0.4641, Val Acc: 84.3333\n",
      "\n",
      "Starting epoch 391 / 500\n",
      "Epoch 391/500, Tr Loss: 0.3309, Tr Acc: 99.8333, Val Loss: 0.4720, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 392 / 500\n",
      "Epoch 392/500, Tr Loss: 0.3400, Tr Acc: 98.8333, Val Loss: 0.4756, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 393 / 500\n",
      "Epoch 393/500, Tr Loss: 0.3302, Tr Acc: 99.8333, Val Loss: 0.4866, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 394 / 500\n",
      "Epoch 394/500, Tr Loss: 0.3284, Tr Acc: 99.6667, Val Loss: 0.4730, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 395 / 500\n",
      "Epoch 395/500, Tr Loss: 0.3280, Tr Acc: 99.6667, Val Loss: 0.4639, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 396 / 500\n",
      "Epoch 396/500, Tr Loss: 0.3348, Tr Acc: 99.0000, Val Loss: 0.4752, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 397 / 500\n",
      "Epoch 397/500, Tr Loss: 0.3263, Tr Acc: 99.8333, Val Loss: 0.4833, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 398 / 500\n",
      "Epoch 398/500, Tr Loss: 0.3287, Tr Acc: 99.8333, Val Loss: 0.4705, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 399 / 500\n",
      "Epoch 399/500, Tr Loss: 0.3296, Tr Acc: 99.6667, Val Loss: 0.4669, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 400 / 500\n",
      "Epoch 400/500, Tr Loss: 0.3330, Tr Acc: 99.1667, Val Loss: 0.4716, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 401 / 500\n",
      "Epoch 401/500, Tr Loss: 0.3323, Tr Acc: 99.5000, Val Loss: 0.4791, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 402 / 500\n",
      "Epoch 402/500, Tr Loss: 0.3349, Tr Acc: 99.1667, Val Loss: 0.4820, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 403 / 500\n",
      "Epoch 403/500, Tr Loss: 0.3328, Tr Acc: 99.3333, Val Loss: 0.4682, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 404 / 500\n",
      "Epoch 404/500, Tr Loss: 0.3299, Tr Acc: 99.5000, Val Loss: 0.4791, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 405 / 500\n",
      "Epoch 405/500, Tr Loss: 0.3244, Tr Acc: 100.0000, Val Loss: 0.4721, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 406 / 500\n",
      "Epoch 406/500, Tr Loss: 0.3269, Tr Acc: 99.6667, Val Loss: 0.4747, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 407 / 500\n",
      "Epoch 407/500, Tr Loss: 0.3305, Tr Acc: 99.8333, Val Loss: 0.4740, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 408 / 500\n",
      "Epoch 408/500, Tr Loss: 0.3247, Tr Acc: 99.8333, Val Loss: 0.4809, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 409 / 500\n",
      "Epoch 409/500, Tr Loss: 0.3261, Tr Acc: 100.0000, Val Loss: 0.4769, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 410 / 500\n",
      "Epoch 410/500, Tr Loss: 0.3273, Tr Acc: 99.8333, Val Loss: 0.4841, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 411 / 500\n",
      "Epoch 411/500, Tr Loss: 0.3262, Tr Acc: 99.8333, Val Loss: 0.4779, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 412 / 500\n",
      "Epoch 412/500, Tr Loss: 0.3253, Tr Acc: 100.0000, Val Loss: 0.4780, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 413 / 500\n",
      "Epoch 413/500, Tr Loss: 0.3274, Tr Acc: 99.6667, Val Loss: 0.4841, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 414 / 500\n",
      "Epoch 414/500, Tr Loss: 0.3264, Tr Acc: 100.0000, Val Loss: 0.4865, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 415 / 500\n",
      "Epoch 415/500, Tr Loss: 0.3265, Tr Acc: 99.8333, Val Loss: 0.4716, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 416 / 500\n",
      "Epoch 416/500, Tr Loss: 0.3265, Tr Acc: 99.8333, Val Loss: 0.4900, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 417 / 500\n",
      "Epoch 417/500, Tr Loss: 0.3296, Tr Acc: 99.5000, Val Loss: 0.4715, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 418 / 500\n",
      "Epoch 418/500, Tr Loss: 0.3252, Tr Acc: 100.0000, Val Loss: 0.4712, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 419 / 500\n",
      "Epoch 419/500, Tr Loss: 0.3299, Tr Acc: 99.3333, Val Loss: 0.4758, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 420 / 500\n",
      "Epoch 420/500, Tr Loss: 0.3258, Tr Acc: 100.0000, Val Loss: 0.4836, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 421 / 500\n",
      "Epoch 421/500, Tr Loss: 0.3286, Tr Acc: 99.6667, Val Loss: 0.4677, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 422 / 500\n",
      "Epoch 422/500, Tr Loss: 0.3286, Tr Acc: 99.6667, Val Loss: 0.4664, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 423 / 500\n",
      "Epoch 423/500, Tr Loss: 0.3276, Tr Acc: 100.0000, Val Loss: 0.4723, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 424 / 500\n",
      "Epoch 424/500, Tr Loss: 0.3305, Tr Acc: 99.6667, Val Loss: 0.4870, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 425 / 500\n",
      "Epoch 425/500, Tr Loss: 0.3267, Tr Acc: 100.0000, Val Loss: 0.4886, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 426 / 500\n",
      "Epoch 426/500, Tr Loss: 0.3278, Tr Acc: 99.8333, Val Loss: 0.4773, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 427 / 500\n",
      "Epoch 427/500, Tr Loss: 0.3228, Tr Acc: 100.0000, Val Loss: 0.4819, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 428 / 500\n",
      "Epoch 428/500, Tr Loss: 0.3271, Tr Acc: 99.5000, Val Loss: 0.4907, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 429 / 500\n",
      "Epoch 429/500, Tr Loss: 0.3276, Tr Acc: 100.0000, Val Loss: 0.4813, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 430 / 500\n",
      "Epoch 430/500, Tr Loss: 0.3376, Tr Acc: 99.0000, Val Loss: 0.4859, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 431 / 500\n",
      "Epoch 431/500, Tr Loss: 0.3260, Tr Acc: 99.8333, Val Loss: 0.4858, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 432 / 500\n",
      "Epoch 432/500, Tr Loss: 0.3297, Tr Acc: 99.6667, Val Loss: 0.4762, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 433 / 500\n",
      "Epoch 433/500, Tr Loss: 0.3308, Tr Acc: 99.8333, Val Loss: 0.4801, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 434 / 500\n",
      "Epoch 434/500, Tr Loss: 0.3248, Tr Acc: 99.8333, Val Loss: 0.4746, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 435 / 500\n",
      "Epoch 435/500, Tr Loss: 0.3356, Tr Acc: 99.0000, Val Loss: 0.4804, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 436 / 500\n",
      "Epoch 436/500, Tr Loss: 0.3353, Tr Acc: 99.3333, Val Loss: 0.4850, Val Acc: 81.0000\n",
      "\n",
      "Starting epoch 437 / 500\n",
      "Epoch 437/500, Tr Loss: 0.3253, Tr Acc: 99.8333, Val Loss: 0.4801, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 438 / 500\n",
      "Epoch 438/500, Tr Loss: 0.3272, Tr Acc: 99.5000, Val Loss: 0.4820, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 439 / 500\n",
      "Epoch 439/500, Tr Loss: 0.3278, Tr Acc: 99.6667, Val Loss: 0.4728, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 440 / 500\n",
      "Epoch 440/500, Tr Loss: 0.3265, Tr Acc: 99.8333, Val Loss: 0.4843, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 441 / 500\n",
      "Epoch 441/500, Tr Loss: 0.3256, Tr Acc: 100.0000, Val Loss: 0.4819, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 442 / 500\n",
      "Epoch 442/500, Tr Loss: 0.3292, Tr Acc: 99.3333, Val Loss: 0.4927, Val Acc: 81.3333\n",
      "\n",
      "Starting epoch 443 / 500\n",
      "Epoch 443/500, Tr Loss: 0.3308, Tr Acc: 99.3333, Val Loss: 0.4830, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 444 / 500\n",
      "Epoch 444/500, Tr Loss: 0.3221, Tr Acc: 100.0000, Val Loss: 0.4715, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 445 / 500\n",
      "Epoch 445/500, Tr Loss: 0.3238, Tr Acc: 100.0000, Val Loss: 0.4772, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 446 / 500\n",
      "Epoch 446/500, Tr Loss: 0.3267, Tr Acc: 99.6667, Val Loss: 0.4787, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 447 / 500\n",
      "Epoch 447/500, Tr Loss: 0.3263, Tr Acc: 99.6667, Val Loss: 0.4672, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 448 / 500\n",
      "Epoch 448/500, Tr Loss: 0.3293, Tr Acc: 99.5000, Val Loss: 0.4774, Val Acc: 82.0000\n",
      "\n",
      "Starting epoch 449 / 500\n",
      "Epoch 449/500, Tr Loss: 0.3242, Tr Acc: 100.0000, Val Loss: 0.4872, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 450 / 500\n",
      "Epoch 450/500, Tr Loss: 0.3245, Tr Acc: 100.0000, Val Loss: 0.4694, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 451 / 500\n",
      "Epoch 451/500, Tr Loss: 0.3220, Tr Acc: 100.0000, Val Loss: 0.4752, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 452 / 500\n",
      "Epoch 452/500, Tr Loss: 0.3333, Tr Acc: 99.1667, Val Loss: 0.4766, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 453 / 500\n",
      "Epoch 453/500, Tr Loss: 0.3298, Tr Acc: 99.3333, Val Loss: 0.4835, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 454 / 500\n",
      "Epoch 454/500, Tr Loss: 0.3248, Tr Acc: 100.0000, Val Loss: 0.4829, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 455 / 500\n",
      "Epoch 455/500, Tr Loss: 0.3268, Tr Acc: 99.6667, Val Loss: 0.4778, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 456 / 500\n",
      "Epoch 456/500, Tr Loss: 0.3219, Tr Acc: 100.0000, Val Loss: 0.4711, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 457 / 500\n",
      "Epoch 457/500, Tr Loss: 0.3238, Tr Acc: 100.0000, Val Loss: 0.4772, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 458 / 500\n",
      "Epoch 458/500, Tr Loss: 0.3261, Tr Acc: 99.8333, Val Loss: 0.4754, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 459 / 500\n",
      "Epoch 459/500, Tr Loss: 0.3254, Tr Acc: 100.0000, Val Loss: 0.4778, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 460 / 500\n",
      "Epoch 460/500, Tr Loss: 0.3279, Tr Acc: 99.6667, Val Loss: 0.4681, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 461 / 500\n",
      "Epoch 461/500, Tr Loss: 0.3266, Tr Acc: 99.8333, Val Loss: 0.4737, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 462 / 500\n",
      "Epoch 462/500, Tr Loss: 0.3363, Tr Acc: 98.8333, Val Loss: 0.4829, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 463 / 500\n",
      "Epoch 463/500, Tr Loss: 0.3238, Tr Acc: 100.0000, Val Loss: 0.4790, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 464 / 500\n",
      "Epoch 464/500, Tr Loss: 0.3271, Tr Acc: 99.6667, Val Loss: 0.4826, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 465 / 500\n",
      "Epoch 465/500, Tr Loss: 0.3234, Tr Acc: 99.8333, Val Loss: 0.4769, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 466 / 500\n",
      "Epoch 466/500, Tr Loss: 0.3239, Tr Acc: 100.0000, Val Loss: 0.4787, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 467 / 500\n",
      "Epoch 467/500, Tr Loss: 0.3253, Tr Acc: 99.5000, Val Loss: 0.4679, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 468 / 500\n",
      "Epoch 468/500, Tr Loss: 0.3230, Tr Acc: 100.0000, Val Loss: 0.4732, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 469 / 500\n",
      "Epoch 469/500, Tr Loss: 0.3221, Tr Acc: 100.0000, Val Loss: 0.4726, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 470 / 500\n",
      "Epoch 470/500, Tr Loss: 0.3246, Tr Acc: 100.0000, Val Loss: 0.4788, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 471 / 500\n",
      "Epoch 471/500, Tr Loss: 0.3237, Tr Acc: 99.8333, Val Loss: 0.4726, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 472 / 500\n",
      "Epoch 472/500, Tr Loss: 0.3316, Tr Acc: 99.8333, Val Loss: 0.4756, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 473 / 500\n",
      "Epoch 473/500, Tr Loss: 0.3292, Tr Acc: 99.3333, Val Loss: 0.4740, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 474 / 500\n",
      "Epoch 474/500, Tr Loss: 0.3259, Tr Acc: 99.8333, Val Loss: 0.4802, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 475 / 500\n",
      "Epoch 475/500, Tr Loss: 0.3275, Tr Acc: 99.5000, Val Loss: 0.4758, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 476 / 500\n",
      "Epoch 476/500, Tr Loss: 0.3234, Tr Acc: 100.0000, Val Loss: 0.4704, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 477 / 500\n",
      "Epoch 477/500, Tr Loss: 0.3249, Tr Acc: 99.8333, Val Loss: 0.4785, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 478 / 500\n",
      "Epoch 478/500, Tr Loss: 0.3239, Tr Acc: 99.8333, Val Loss: 0.4820, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 479 / 500\n",
      "Epoch 479/500, Tr Loss: 0.3288, Tr Acc: 99.5000, Val Loss: 0.4703, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 480 / 500\n",
      "Epoch 480/500, Tr Loss: 0.3230, Tr Acc: 99.8333, Val Loss: 0.4670, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 481 / 500\n",
      "Epoch 481/500, Tr Loss: 0.3243, Tr Acc: 99.8333, Val Loss: 0.4883, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 482 / 500\n",
      "Epoch 482/500, Tr Loss: 0.3259, Tr Acc: 99.8333, Val Loss: 0.4825, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 483 / 500\n",
      "Epoch 483/500, Tr Loss: 0.3208, Tr Acc: 100.0000, Val Loss: 0.4756, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 484 / 500\n",
      "Epoch 484/500, Tr Loss: 0.3222, Tr Acc: 100.0000, Val Loss: 0.4656, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 485 / 500\n",
      "Epoch 485/500, Tr Loss: 0.3228, Tr Acc: 100.0000, Val Loss: 0.4790, Val Acc: 83.6667\n",
      "\n",
      "Starting epoch 486 / 500\n",
      "Epoch 486/500, Tr Loss: 0.3271, Tr Acc: 99.8333, Val Loss: 0.4875, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 487 / 500\n",
      "Epoch 487/500, Tr Loss: 0.3297, Tr Acc: 99.5000, Val Loss: 0.4800, Val Acc: 81.6667\n",
      "\n",
      "Starting epoch 488 / 500\n",
      "Epoch 488/500, Tr Loss: 0.3272, Tr Acc: 99.6667, Val Loss: 0.4846, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 489 / 500\n",
      "Epoch 489/500, Tr Loss: 0.3248, Tr Acc: 99.8333, Val Loss: 0.4807, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 490 / 500\n",
      "Epoch 490/500, Tr Loss: 0.3277, Tr Acc: 99.3333, Val Loss: 0.4744, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 491 / 500\n",
      "Epoch 491/500, Tr Loss: 0.3214, Tr Acc: 100.0000, Val Loss: 0.4783, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 492 / 500\n",
      "Epoch 492/500, Tr Loss: 0.3273, Tr Acc: 99.6667, Val Loss: 0.4851, Val Acc: 83.3333\n",
      "\n",
      "Starting epoch 493 / 500\n",
      "Epoch 493/500, Tr Loss: 0.3230, Tr Acc: 100.0000, Val Loss: 0.4662, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 494 / 500\n",
      "Epoch 494/500, Tr Loss: 0.3235, Tr Acc: 100.0000, Val Loss: 0.4809, Val Acc: 82.3333\n",
      "\n",
      "Starting epoch 495 / 500\n",
      "Epoch 495/500, Tr Loss: 0.3249, Tr Acc: 100.0000, Val Loss: 0.4683, Val Acc: 82.6667\n",
      "\n",
      "Starting epoch 496 / 500\n",
      "Epoch 496/500, Tr Loss: 0.3229, Tr Acc: 100.0000, Val Loss: 0.4790, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 497 / 500\n",
      "Epoch 497/500, Tr Loss: 0.3248, Tr Acc: 99.6667, Val Loss: 0.4826, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 498 / 500\n",
      "Epoch 498/500, Tr Loss: 0.3248, Tr Acc: 100.0000, Val Loss: 0.4691, Val Acc: 83.0000\n",
      "\n",
      "Starting epoch 499 / 500\n",
      "Epoch 499/500, Tr Loss: 0.3217, Tr Acc: 100.0000, Val Loss: 0.4671, Val Acc: 84.6667\n",
      "\n",
      "Starting epoch 500 / 500\n",
      "Epoch 500/500, Tr Loss: 0.3271, Tr Acc: 99.6667, Val Loss: 0.4692, Val Acc: 83.6667\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from common import train\n",
    "\n",
    "lr = 0.00001\n",
    "n_epochs = 2000\n",
    "patience = 200\n",
    "#,weight_decay=1e-2\n",
    "weight_name = \"Physionet_hopefullnet_C3C4_executedImagine_Wandb\"\n",
    "model.to(device=device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate,amsgrad=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_model,train_loss,valid_loss,train_acc,valid_acc = train(model, train_iterator, valid_iterator, optimizer,criterion,device,wand)\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss0.32075140664452\n",
      "Valid loss0.4639082133769989\n"
     ]
    }
   ],
   "source": [
    "def do_plot(train_loss, valid_loss):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='valid_loss')\n",
    "    plt.title('loss {}'.format(iter))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss,'r')\n",
    "plt.plot(valid_loss,'b')\n",
    "plt.show()\n",
    "print(\"Train loss\" + str(min(train_loss)))\n",
    "print(\"Valid loss\" + str(min(valid_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 99.66667175292969\n",
      "100.0\n",
      "96.0209453125\n",
      "Valid acc:83.66666666666667\n",
      "85.33333333333334\n",
      "80.3906666666667\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print('Train acc: '+ str(train_acc[-1].item()))\n",
    "print(max(train_acc).item())\n",
    "print(sum(train_acc).item()/len(train_acc))\n",
    "\n",
    "print('Valid acc:' + str(valid_acc[-1].item()))\n",
    "print(max(valid_acc).item())\n",
    "print(sum(valid_acc).item()/len(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>█████▇█▇▇▅▅▅▆▄▆▄▄▄▂▅▄▄▂▃▅▂▃▃▄▁▅▃▂▆▄▁▃▃▄▃</td></tr><tr><td>val/val_accuracy</td><td>▁▁▃▄▃▆▆▆▅▇▅▇▇▆▇▇▆▇▅▇▇▆█▇▇▅▇▇▇██▇██▇█████</td></tr><tr><td>val/val_loss</td><td>████▇▇▆▅▅▄▇▄▄▄▃▁▃▂▆▂▂▄▂▂▂▆▃▂▂▂▁▃▂▁▃▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>0.37556</td></tr><tr><td>val/val_accuracy</td><td>76.33333</td></tr><tr><td>val/val_loss</td><td>0.55761</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hopefullnet_C3C4_executedImagine_Wandb</strong>: <a href=\"https://wandb.ai/newturno/Motor-Imagery/runs/1an29igp\" target=\"_blank\">https://wandb.ai/newturno/Motor-Imagery/runs/1an29igp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220309_104709-1an29igp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🐝 Close your wandb run \n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ae246934a7cd8e2aa0ce1a37b33766718792e8e777147d6c6548b2d965aeb7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
