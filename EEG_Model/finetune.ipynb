{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import setup_dataflow,EEG,getepoch,do_plot,create_dataloader\n",
    "import wandb\n",
    "from mne.datasets import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvNet,CNN2D,ConvNet_physionet\n",
    "from torchsummary import summary\n",
    "net = ConvNet().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute\n",
    "#net.load_state_dict(torch.load('./save_weight/sunsun/sunsun_85.1852'))\n",
    "\n",
    "# MI\n",
    "net.load_state_dict(torch.load('./save_weight/Beau_S48_MI/0.6492_Beau_S48_MI_0.6492_82.6087'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layer1.0.weight', 'layer1.0.bias', 'layer1.1.weight', 'layer1.1.bias', 'layer1.1.running_mean', 'layer1.1.running_var', 'layer1.1.num_batches_tracked', 'layer2.0.weight', 'layer2.0.bias', 'layer2.1.weight', 'layer2.1.bias', 'layer2.1.running_mean', 'layer2.1.running_var', 'layer2.1.num_batches_tracked', 'layer3.0.weight', 'layer3.0.bias', 'layer3.1.weight', 'layer3.1.bias', 'layer3.1.running_mean', 'layer3.1.running_var', 'layer3.1.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms = net.state_dict()\n",
    "parms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad and 'layer1' in name:\n",
    "        param.requires_grad = False\n",
    "    if param.requires_grad and 'layer2' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/EEG_Model/dataset/finetune_EEG/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R09.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fz', 'C3', 'Cz', 'C4', 'Pz', 'PO7', 'Oz', 'PO8', 'STIM MARKERS']\n",
      "250.0\n",
      "['C3', 'C4', 'STIM MARKERS']\n",
      "['Fz', 'C3', 'Cz', 'C4', 'Pz', 'PO7', 'Oz', 'PO8', 'STIM MARKERS']\n",
      "250.0\n",
      "['C3', 'C4', 'STIM MARKERS']\n",
      "Raw done\n",
      "Filtering raw data in 4 contiguous segments\n",
      "Setting up band-pass filter from 8 - 14 Hz\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/root/EEG_Model/common.py:255: RuntimeWarning: This filename (/root/EEG_Model/dataset/finetune_EEG/S008/S008R09.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 14.00 Hz\n",
      "- Upper transition bandwidth: 3.50 Hz (-6 dB cutoff frequency: 15.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filter done\n",
      "240 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 80 events and 1751 original time points ...\n",
      "4 bad epochs dropped\n",
      "(76, 2, 1751)\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/EEG_Model/common.py:280: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "path = \"/root/EEG_Model/dataset/finetune_EEG/\"\n",
    "#subject to run\n",
    "#runs = [3,5,7,9]\n",
    "runs = [4,6,8,10]\n",
    "subjects = [8]\n",
    "#recorded eeg class\n",
    "eeg = EEG(path, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "# apply filter \n",
    "raw=raw.notch_filter([50,75,100])\n",
    "raw=raw.filter( 8,14, method='fir', verbose=20)\n",
    "print(\"Filter done\")\n",
    "\n",
    "epochs=eeg.epochs(raw,tmin=0,tmax=7,baseline=(0,2))\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "X, y = eeg.get_X_y(epochs)\n",
    "print(X.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (53, 2, 1751) (53,)\n",
      "Test size (23, 2, 1751) (23,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "\n",
    "print('train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1751\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnewturno\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/EEG_Model/wandb/run-20230114_183814-2oyadjgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/newturno/Motor-Imagery/runs/2oyadjgy\" target=\"_blank\">CNN_SunsunFine_S8_EX</a></strong> to <a href=\"https://wandb.ai/newturno/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"CNN_SunsunFine_S8_EX\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 300000,\n",
    "      \"weightname\":\"S8_SunsunFine_MI\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300000, Tr Loss: 2.3030, Tr Acc: 58.4906, Val Loss: 6.8469, Val Acc: 52.1739\n",
      "Epoch 101/300000, Tr Loss: 2.1263, Tr Acc: 56.6038, Val Loss: 2.7438, Val Acc: 52.1739\n",
      "Epoch 201/300000, Tr Loss: 2.0105, Tr Acc: 62.2642, Val Loss: 2.7722, Val Acc: 52.1739\n",
      "Epoch 301/300000, Tr Loss: 2.0464, Tr Acc: 56.6038, Val Loss: 2.7935, Val Acc: 52.1739\n",
      "Epoch 401/300000, Tr Loss: 1.8605, Tr Acc: 56.6038, Val Loss: 2.8118, Val Acc: 52.1739\n",
      "Epoch 501/300000, Tr Loss: 2.0534, Tr Acc: 56.6038, Val Loss: 2.8282, Val Acc: 52.1739\n",
      "Epoch 601/300000, Tr Loss: 1.9489, Tr Acc: 60.3774, Val Loss: 2.8446, Val Acc: 52.1739\n",
      "Epoch 701/300000, Tr Loss: 1.9140, Tr Acc: 56.6038, Val Loss: 2.8483, Val Acc: 52.1739\n",
      "Epoch 801/300000, Tr Loss: 1.6967, Tr Acc: 60.3774, Val Loss: 2.8609, Val Acc: 52.1739\n",
      "Epoch 901/300000, Tr Loss: 1.7725, Tr Acc: 56.6038, Val Loss: 2.8659, Val Acc: 52.1739\n",
      "Epoch 1001/300000, Tr Loss: 1.6981, Tr Acc: 60.3774, Val Loss: 2.8699, Val Acc: 52.1739\n",
      "Epoch 1101/300000, Tr Loss: 1.6007, Tr Acc: 60.3774, Val Loss: 2.8769, Val Acc: 52.1739\n",
      "Epoch 1201/300000, Tr Loss: 1.4291, Tr Acc: 62.2642, Val Loss: 2.8730, Val Acc: 52.1739\n",
      "Epoch 1301/300000, Tr Loss: 1.6571, Tr Acc: 66.0377, Val Loss: 2.8789, Val Acc: 52.1739\n",
      "Epoch 1401/300000, Tr Loss: 1.6609, Tr Acc: 58.4906, Val Loss: 2.8736, Val Acc: 52.1739\n",
      "Epoch 1501/300000, Tr Loss: 1.3116, Tr Acc: 60.3774, Val Loss: 2.8776, Val Acc: 52.1739\n",
      "Epoch 1601/300000, Tr Loss: 1.4642, Tr Acc: 64.1509, Val Loss: 2.8777, Val Acc: 52.1739\n",
      "Epoch 1701/300000, Tr Loss: 1.1620, Tr Acc: 64.1509, Val Loss: 2.8771, Val Acc: 52.1739\n",
      "Epoch 1801/300000, Tr Loss: 1.2191, Tr Acc: 66.0377, Val Loss: 2.8755, Val Acc: 52.1739\n",
      "Epoch 1901/300000, Tr Loss: 1.2203, Tr Acc: 69.8113, Val Loss: 2.8738, Val Acc: 52.1739\n",
      "Epoch 2001/300000, Tr Loss: 1.1494, Tr Acc: 62.2642, Val Loss: 2.8673, Val Acc: 52.1739\n",
      "Epoch 2101/300000, Tr Loss: 1.0166, Tr Acc: 73.5849, Val Loss: 2.8694, Val Acc: 52.1739\n",
      "Epoch 2201/300000, Tr Loss: 1.2261, Tr Acc: 64.1509, Val Loss: 2.8676, Val Acc: 52.1739\n",
      "Epoch 2301/300000, Tr Loss: 1.2678, Tr Acc: 66.0377, Val Loss: 2.8621, Val Acc: 52.1739\n",
      "Epoch 2401/300000, Tr Loss: 1.0513, Tr Acc: 67.9245, Val Loss: 2.8591, Val Acc: 52.1739\n",
      "Epoch 2501/300000, Tr Loss: 1.1236, Tr Acc: 67.9245, Val Loss: 2.8519, Val Acc: 52.1739\n",
      "Epoch 2601/300000, Tr Loss: 1.1545, Tr Acc: 66.0377, Val Loss: 2.8489, Val Acc: 52.1739\n",
      "Epoch 2701/300000, Tr Loss: 1.0980, Tr Acc: 69.8113, Val Loss: 2.8474, Val Acc: 52.1739\n",
      "Epoch 2801/300000, Tr Loss: 0.9140, Tr Acc: 69.8113, Val Loss: 2.8397, Val Acc: 52.1739\n",
      "Epoch 2901/300000, Tr Loss: 0.9908, Tr Acc: 67.9245, Val Loss: 2.8341, Val Acc: 52.1739\n",
      "Epoch 3001/300000, Tr Loss: 0.9063, Tr Acc: 69.8113, Val Loss: 2.8301, Val Acc: 52.1739\n",
      "Epoch 3101/300000, Tr Loss: 0.7329, Tr Acc: 71.6981, Val Loss: 2.8245, Val Acc: 52.1739\n",
      "Epoch 3201/300000, Tr Loss: 0.8359, Tr Acc: 71.6981, Val Loss: 2.8182, Val Acc: 52.1739\n",
      "Epoch 3301/300000, Tr Loss: 0.7652, Tr Acc: 67.9245, Val Loss: 2.8103, Val Acc: 52.1739\n",
      "Epoch 3401/300000, Tr Loss: 0.8026, Tr Acc: 79.2453, Val Loss: 2.8081, Val Acc: 52.1739\n",
      "Epoch 3501/300000, Tr Loss: 0.6469, Tr Acc: 71.6981, Val Loss: 2.8008, Val Acc: 52.1739\n",
      "Epoch 3601/300000, Tr Loss: 0.7397, Tr Acc: 71.6981, Val Loss: 2.7971, Val Acc: 52.1739\n",
      "Epoch 3701/300000, Tr Loss: 0.6560, Tr Acc: 75.4717, Val Loss: 2.7874, Val Acc: 52.1739\n",
      "Epoch 3801/300000, Tr Loss: 0.6509, Tr Acc: 75.4717, Val Loss: 2.7808, Val Acc: 52.1739\n",
      "Epoch 3901/300000, Tr Loss: 0.5588, Tr Acc: 81.1321, Val Loss: 2.7733, Val Acc: 52.1739\n",
      "Epoch 4001/300000, Tr Loss: 0.6376, Tr Acc: 71.6981, Val Loss: 2.7660, Val Acc: 52.1739\n",
      "Epoch 4101/300000, Tr Loss: 0.6146, Tr Acc: 71.6981, Val Loss: 2.7586, Val Acc: 52.1739\n",
      "Epoch 4201/300000, Tr Loss: 0.5751, Tr Acc: 79.2453, Val Loss: 2.7534, Val Acc: 52.1739\n",
      "Epoch 4301/300000, Tr Loss: 0.4921, Tr Acc: 77.3585, Val Loss: 2.7490, Val Acc: 52.1739\n",
      "Epoch 4401/300000, Tr Loss: 0.4879, Tr Acc: 79.2453, Val Loss: 2.7409, Val Acc: 52.1739\n",
      "Epoch 4501/300000, Tr Loss: 0.5618, Tr Acc: 75.4717, Val Loss: 2.7388, Val Acc: 52.1739\n",
      "Epoch 4601/300000, Tr Loss: 0.4727, Tr Acc: 79.2453, Val Loss: 2.7341, Val Acc: 52.1739\n",
      "Epoch 4701/300000, Tr Loss: 0.4940, Tr Acc: 79.2453, Val Loss: 2.7248, Val Acc: 52.1739\n",
      "Epoch 4801/300000, Tr Loss: 0.4209, Tr Acc: 79.2453, Val Loss: 2.7212, Val Acc: 52.1739\n",
      "Epoch 4901/300000, Tr Loss: 0.4281, Tr Acc: 83.0189, Val Loss: 2.7140, Val Acc: 52.1739\n",
      "Epoch 5001/300000, Tr Loss: 0.3340, Tr Acc: 86.7925, Val Loss: 2.7103, Val Acc: 52.1739\n",
      "Epoch 5101/300000, Tr Loss: 0.3068, Tr Acc: 88.6792, Val Loss: 2.7052, Val Acc: 52.1739\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      5\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_loss,valid_loss,train_accuracy,valid_accuracy \u001b[39m=\u001b[39mtrain(\n\u001b[1;32m      8\u001b[0m     model \u001b[39m=\u001b[39;49m net,\n\u001b[1;32m      9\u001b[0m     loader_train \u001b[39m=\u001b[39;49m train_loader,\n\u001b[1;32m     10\u001b[0m     loader_test \u001b[39m=\u001b[39;49m test_loader,\n\u001b[1;32m     11\u001b[0m     vail_loader \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     12\u001b[0m     optimizer \u001b[39m=\u001b[39;49m optimizer  ,\n\u001b[1;32m     13\u001b[0m     criterion \u001b[39m=\u001b[39;49m criterion ,\n\u001b[1;32m     14\u001b[0m     device \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m     wand \u001b[39m=\u001b[39;49m wand\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m wandb\u001b[39m.\u001b[39malert(\n\u001b[1;32m     19\u001b[0m             title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinish\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m             text\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinishing training\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m         )\n",
      "File \u001b[0;32m~/EEG_Model/common.py:130\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader_train, loader_test, optimizer, criterion, device, wand, vail_loader, cross)\u001b[0m\n\u001b[1;32m    125\u001b[0m train_loss\u001b[39m.\u001b[39mappend(iter_loss\u001b[39m/\u001b[39miterations)\n\u001b[1;32m    128\u001b[0m train_accuracy\u001b[39m.\u001b[39mappend((\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m correct\u001b[39m.\u001b[39mfloat() \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loader_train\u001b[39m.\u001b[39mdataset)))\n\u001b[1;32m    129\u001b[0m train_metrics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtrain/train_loss\u001b[39m\u001b[39m\"\u001b[39m: iter_loss\u001b[39m/\u001b[39miterations, \n\u001b[0;32m--> 130\u001b[0m                \u001b[39m\"\u001b[39m\u001b[39mtrain/train_accuracy\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m correct\u001b[39m.\u001b[39;49mfloat() \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loader_train\u001b[39m.\u001b[39mdataset))}\n\u001b[1;32m    132\u001b[0m wand\u001b[39m.\u001b[39mlog({\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmetrics, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_metrics})\n\u001b[1;32m    134\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from common import train\n",
    "config = wand.config\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_loader,\n",
    "    loader_test = test_loader,\n",
    "    vail_loader = None,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ee62090f476f7f208daa0d546a5a64db59508b1a22febc715667ce49424855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
